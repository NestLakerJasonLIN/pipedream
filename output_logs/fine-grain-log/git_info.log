45ba8eb129f6d70a3eebe590a72b1936b00aa735
diff --git a/runtime/image_classification/driver_configs/vgg16_2mp_exp.yml b/runtime/image_classification/driver_configs/vgg16_2mp_exp.yml
index 9071b4a..36ff2f0 100644
--- a/runtime/image_classification/driver_configs/vgg16_2mp_exp.yml
+++ b/runtime/image_classification/driver_configs/vgg16_2mp_exp.yml
@@ -3,7 +3,7 @@
 'data_dir': '/home/ubuntu/data/imagenet_tiny/'
 'config_file': 'models/vgg16/gpus=2/mp_conf.json'
 'container': 'nvcr.io/nvidia/pytorch:19.05-py3'
-'machines': ['HOST0:0', 'HOST1:1']
+'machines': ['172.31.73.214:0', '172.31.71.248:1']
 'batch_size': 64
 'learning_rate': 0.01
 'weight_decay': 0.0005
diff --git a/runtime/image_classification/models/vgg16/gpus=2/stage0.py b/runtime/image_classification/models/vgg16/gpus=2/stage0.py
index f60da29..1f37860 100644
--- a/runtime/image_classification/models/vgg16/gpus=2/stage0.py
+++ b/runtime/image_classification/models/vgg16/gpus=2/stage0.py
@@ -2,7 +2,9 @@
 # Licensed under the MIT license.
 
 import torch
-from datetime import datetime
+import sys
+sys.path.append("/home/ubuntu/pipedream/runtime")
+from runtime_utilities import t_start, t_stop
 
 class Stage0(torch.nn.Module):
     def __init__(self):
@@ -19,28 +21,47 @@ class Stage0(torch.nn.Module):
         self._initialize_weights()
 
     def forward(self, input0, forward_minibatch_id, backward_minibatch_id, comm_handler):
-        start_time = datetime.now()
+        start_time = t_start()
     
+        start_time_clone = t_start()
         out0 = input0.clone()
+        t_stop(start_time_clone, "clone:")
+
+        start_time_clone = t_start()
         out2 = self.layer2(out0)
+        t_stop(start_time_clone, "out2:")
+
+        start_time_clone = t_start()
         out3 = self.layer3(out2)
+        t_stop(start_time_clone, "out3:")
+
+        start_time_clone = t_start()
         out4 = self.layer4(out3)
+        t_stop(start_time_clone, "out4:")
+
+        start_time_clone = t_start()
         out5 = self.layer5(out4)
+        t_stop(start_time_clone, "out5:")
+
+        start_time_clone = t_start()
         out6 = self.layer6(out5)
+        t_stop(start_time_clone, "out6:")
+
+        start_time_clone = t_start()
         out7 = self.layer7(out6)
+        t_stop(start_time_clone, "out7:")
+
+        start_time_clone = t_start()
         out8 = self.layer8(out7)
+        t_stop(start_time_clone, "out8:")
 
-        dt = datetime.now() - start_time
-        elapsed = (dt.days * 24 * 60 * 60 + dt.seconds) * 1000 + dt.microseconds / 1000.0
-        print("Stage0 before last layer:", "%.20fms" % elapsed)
+        t_stop(start_time, "other layers:")
 
-        start_time = datetime.now()
+        start_time = t_start()
         
         out9 = self.upstream_tail(out8, forward_minibatch_id, backward_minibatch_id, comm_handler)
         
-        dt = datetime.now() - start_time
-        elapsed = (dt.days * 24 * 60 * 60 + dt.seconds) * 1000 + dt.microseconds / 1000.0
-        print("Stage0 last layer:", "%.20fms" % elapsed)
+        t_stop(start_time, "upstream tail:")
         
         return out9
 
@@ -73,13 +94,13 @@ class Upstream_Tail(torch.nn.Module):
         
         block_out_list = []
         
-        print("Stage0 Upstream_Tail:")
+        print(" -> Stage0 Upstream_Tail:")
         inp = self.padder(inp)
         h_pad, w_pad = inp.size(2), inp.size(3)
         block_height, block_width = h_pad // 2,  w_pad // 2
         
         # block_0
-        start_time = datetime.now()
+        start_time = t_start()
 
         h_start, h_end = 0, block_height + self.kernel_size-1
         w_start, w_end = 0, block_width + self.kernel_size-1
@@ -92,12 +113,10 @@ class Upstream_Tail(torch.nn.Module):
         comm_handler.send_block(block_out, forward_minibatch_id=forward_minibatch_id,
                                      backward_minibatch_id=backward_minibatch_id)
 
-        dt = datetime.now() - start_time
-        elapsed = (dt.days * 24 * 60 * 60 + dt.seconds) * 1000 + dt.microseconds / 1000.0
-        print(" ->block 0 time:", "%.20fms" % elapsed)
+        t_stop(start_time, "  -> block 0 time:")
 
         # block_1
-        start_time = datetime.now()
+        start_time = t_start()
 
         h_start, h_end = 0, block_height + self.kernel_size-1
         w_start, w_end = block_width, w_pad
@@ -110,12 +129,10 @@ class Upstream_Tail(torch.nn.Module):
         comm_handler.send_block(block_out, forward_minibatch_id=forward_minibatch_id,
                                      backward_minibatch_id=backward_minibatch_id)
 
-        dt = datetime.now() - start_time
-        elapsed = (dt.days * 24 * 60 * 60 + dt.seconds) * 1000 + dt.microseconds / 1000.0
-        print(" ->block 1 time:", "%.20fms" % elapsed)
+        t_stop(start_time, "  -> block 1 time:")
 
         # block_2
-        start_time = datetime.now()
+        start_time = t_start()
 
         h_start, h_end = block_height, h_pad
         w_start, w_end = 0, block_width + self.kernel_size-1
@@ -128,12 +145,10 @@ class Upstream_Tail(torch.nn.Module):
         comm_handler.send_block(block_out, forward_minibatch_id=forward_minibatch_id,
                              backward_minibatch_id=backward_minibatch_id)
         
-        dt = datetime.now() - start_time
-        elapsed = (dt.days * 24 * 60 * 60 + dt.seconds) * 1000 + dt.microseconds / 1000.0
-        print(" ->block 2 time:", "%.20fms" % elapsed)
+        t_stop(start_time, "  -> block 2 time:")
 
         # block_3
-        start_time = datetime.now()
+        start_time = t_start()
 
         h_start, h_end = block_height, h_pad
         w_start, w_end = block_width, w_pad
@@ -146,9 +161,7 @@ class Upstream_Tail(torch.nn.Module):
         comm_handler.send_block(block_out, forward_minibatch_id=forward_minibatch_id,
                                      backward_minibatch_id=backward_minibatch_id)
 
-        dt = datetime.now() - start_time
-        elapsed = (dt.days * 24 * 60 * 60 + dt.seconds) * 1000 + dt.microseconds / 1000.0
-        print(" ->block 3 time:", "%.20fms" % elapsed)
+        t_stop(start_time, "  -> block 3 time:")
 
         return self._combine(block_out_list)
     
diff --git a/runtime/image_classification/models/vgg16/gpus=2/stage1.py b/runtime/image_classification/models/vgg16/gpus=2/stage1.py
index 9eaa1fe..902f38f 100644
--- a/runtime/image_classification/models/vgg16/gpus=2/stage1.py
+++ b/runtime/image_classification/models/vgg16/gpus=2/stage1.py
@@ -2,7 +2,9 @@
 # Licensed under the MIT license.
 
 import torch
-from datetime import datetime
+import sys
+sys.path.append("/home/ubuntu/pipedream/runtime")
+from runtime_utilities import t_start, t_stop
 
 class Stage1(torch.nn.Module):
     def __init__(self):
@@ -41,15 +43,13 @@ class Stage1(torch.nn.Module):
         self._initialize_weights()
 
     def forward(self, forward_minibatch_id, backward_minibatch_id, r):
-        start_time = datetime.now()
+        start_time = t_start()
         
         out1 = self.downstream_head(forward_minibatch_id, backward_minibatch_id, r)
         
-        dt = datetime.now() - start_time
-        elapsed = (dt.days * 24 * 60 * 60 + dt.seconds) * 1000 + dt.microseconds / 1000.0
-        print("Stage1 1st layer:", "%.20fms" % elapsed)
+        t_stop(start_time, " -> Stage1 1st layer:")
 
-        start_time = datetime.now()
+        start_time = t_start()
         out2 = self.layer2(out1)
         out3 = self.layer3(out2)
         out4 = self.layer4(out3)
@@ -82,9 +82,7 @@ class Stage1(torch.nn.Module):
         out31 = self.layer31(out30)
         out32 = self.layer32(out31)
 
-        dt = datetime.now() - start_time
-        elapsed = (dt.days * 24 * 60 * 60 + dt.seconds) * 1000 + dt.microseconds / 1000.0
-        print("Stage1 other layers:", "%.20fms" % elapsed)
+        t_stop(start_time, " -> Stage1 other layers:")
 
         return out32
 
@@ -108,23 +106,21 @@ class Downstream_Head(torch.nn.Module):
              
     def forward(self, forward_minibatch_id, backward_minibatch_id, r):
 
-        print("Stage1 Downstream_Head:")
+        print(" -> Stage1 Downstream_Head:")
 
         block_num = 4
         block_out_relu = []
         
         for block_id in range(block_num):
-            start_time = datetime.now()
+            start_time = t_start()
             block_inp_relu = r.comm_handler.recv_block(forward_minibatch_id, backward_minibatch_id)
-            dt = datetime.now() - start_time
-            elapsed = (dt.days * 24 * 60 * 60 + dt.seconds) * 1000 + dt.microseconds / 1000.0
-            print(" ->bid:", block_id, "recv elapsed:", "%.20fms" % elapsed)
+            t_stop(start_time, "  -> bid: {} recv elapsed:".format(block_id))
 
             # store block_inp_relu into buffer
             # slice and clone buffer and pass into ReLU
             # return buffer as input_tensor
 
-            start_time = datetime.now()
+            start_time = t_start()
             if (block_id == 0):
                 # infer shape from the first recv block
                 batch_size, channel_size = block_inp_relu.size(0), block_inp_relu.size(1)
@@ -141,20 +137,17 @@ class Downstream_Head(torch.nn.Module):
             else:
                 block_buffer[:, :, 57:, 57:] = block_inp_relu
                 block_out_relu.append(self.relu(block_buffer[:, :, 57:, 57:].clone()))
-            dt = datetime.now() - start_time
-            elapsed = (dt.days * 24 * 60 * 60 + dt.seconds) * 1000 + dt.microseconds / 1000.0
-            print(" ->bid:", block_id, "fill elapsed:", "%.20fms" % elapsed)
+
+            t_stop(start_time, "  -> bid: {} fill elapsed:".format(block_id))
 
 
         # Used to track where to receive forward from.
         r.comm_handler.increment_messaging_index(
             sending=False)
         
-        start_time = datetime.now()
+        start_time = t_start()
         relu_out = self._combine(block_out_relu)
-        dt = datetime.now() - start_time
-        elapsed = (dt.days * 24 * 60 * 60 + dt.seconds) * 1000 + dt.microseconds / 1000.0
-        print(" ->_combine elapsed:", "%.20fms" % elapsed)
+        t_stop(start_time, " -> combine elapsed:".format(block_id))
 
         r.tensors[-1]["out0"] = block_buffer
 
diff --git a/runtime/runtime_block.py b/runtime/runtime_block.py
index 15d28e1..27583f6 100644
--- a/runtime/runtime_block.py
+++ b/runtime/runtime_block.py
@@ -3,7 +3,9 @@
 
 import collections
 import itertools
-import time
+import sys
+sys.path.append("/home/ubuntu/pipedream/runtime")
+from runtime_utilities import t_start, t_stop
 import torch
 import torch.distributed as dist
 
@@ -506,26 +508,20 @@ class StageRuntime:
         """Run forward pass.
         """
         # Receive tensors from previous worker.
-        from datetime import datetime
-
         print("forward_minibatch_id", self.forward_minibatch_id)
 
-        start_time = datetime.now()
+        start_time = t_start()
         self.receive_tensors_forward()
         tensors = self.tensors[-1]
 
-        dt = datetime.now() - start_time
-        elapsed = (dt.days * 24 * 60 * 60 + dt.seconds) * 1000 + dt.microseconds / 1000.0
-        print(" -> recv elapsed:", "%.20fms" % elapsed)
+        t_stop(start_time, " -> recv elapsed:")
 
-        start_time = datetime.now()
+        start_time = t_start()
 
         # Run forward pass.
         self._run_forward(tensors)
 
-        dt = datetime.now() - start_time
-        elapsed = (dt.days * 24 * 60 * 60 + dt.seconds) * 1000 + dt.microseconds / 1000.0
-        print(" -> _run_forward elapsed:", "%.20fms" % elapsed)
+        t_stop(start_time, " -> _run_forward elapsed:")
         print("")
 
         # Send tensors forward.
@@ -565,6 +561,7 @@ class StageRuntime:
                 
                 # stage 0
                 if self.loader_iter is not None:
+                    print("run_forward_ input name:", input_names)
                     module_outputs = module(*[tensors[input_name]
                                           for input_name in input_names],
                                           forward_minibatch_id=self.forward_minibatch_id, 
diff --git a/runtime/runtime_utilities.py b/runtime/runtime_utilities.py
index 192ca31..e380774 100644
--- a/runtime/runtime_utilities.py
+++ b/runtime/runtime_utilities.py
@@ -1,6 +1,8 @@
 # Copyright (c) Microsoft Corporation.
 # Licensed under the MIT license.
 
+import time
+
 class RuntimeStats:
     def __init__(self, forward):
         self.stats = {
@@ -25,4 +27,13 @@ class RuntimeStats:
 
     def reset_stats(self):
         for i in self.stats.keys():
-            self.stats[i] = 0.0
\ No newline at end of file
+            self.stats[i] = 0.0
+
+def t_start():
+    return time.clock_gettime(time.CLOCK_THREAD_CPUTIME_ID)
+
+def t_stop(start_time, prefix="", print_info=True):
+    elapsed = (time.clock_gettime(time.CLOCK_THREAD_CPUTIME_ID) - start_time) * 1000
+    if print_info:
+        print(prefix, "%.20fms" % elapsed)
+    return elapsed
\ No newline at end of file
