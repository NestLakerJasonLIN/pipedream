a31f1d54d29269b91fa88d128fcf76806b43f4fa
diff --git a/archive_logs/2020-04-21T03:37:53/command_history.log b/archive_logs/2020-04-21T03:37:53/command_history.log
deleted file mode 100644
index 3b27b79..0000000
--- a/archive_logs/2020-04-21T03:37:53/command_history.log
+++ /dev/null
@@ -1,2 +0,0 @@
-ssh -n 172.31.14.215 -o StrictHostKeyChecking=no "nvidia-docker run -d -v /home/ubuntu:/home/ubuntu -v /home/ubuntu/pipedream:/home/ubuntu/pipedream --net=host --shm-size 16g nvcr.io/nvidia/pytorch:19.05-py3 /bin/bash -c 'cp /home/ubuntu/pipedream/runtime/launch.py /home/ubuntu/pipedream/runtime/image_classification;  cd /home/ubuntu/pipedream/runtime/image_classification; python -m launch --nnodes 2 --node_rank 0 --nproc_per_node 1 main_with_runtime.py --data_dir /home/ubuntu/data/imagenet/ --master_addr 172.31.14.215 --module models.vgg16.gpus=2 --checkpoint_dir /home/ubuntu/pipedream/output_logs/2020-04-21T03:37:53 --distributed_backend gloo  -b 64 --lr 0.010000 --lr_policy polynomial --weight-decay 0.000500 --epochs 60 --print-freq 10 --verbose 10 --num_ranks_in_server 1 --config_path models/vgg16/gpus=2/mp_conf.json 2>&1 | tee /home/ubuntu/pipedream/output_logs/2020-04-21T03:37:53/output.log.0; rm launch.py'"
-ssh -n 172.31.14.150 -o StrictHostKeyChecking=no "nvidia-docker run -d -v /home/ubuntu:/home/ubuntu -v /home/ubuntu/pipedream:/home/ubuntu/pipedream --net=host --shm-size 16g nvcr.io/nvidia/pytorch:19.05-py3 /bin/bash -c 'cp /home/ubuntu/pipedream/runtime/launch.py /home/ubuntu/pipedream/runtime/image_classification;  cd /home/ubuntu/pipedream/runtime/image_classification; python -m launch --nnodes 2 --node_rank 1 --nproc_per_node 1 main_with_runtime.py --data_dir /home/ubuntu/data/imagenet/ --master_addr 172.31.14.215 --module models.vgg16.gpus=2 --checkpoint_dir /home/ubuntu/pipedream/output_logs/2020-04-21T03:37:53 --distributed_backend gloo  -b 64 --lr 0.010000 --lr_policy polynomial --weight-decay 0.000500 --epochs 60 --print-freq 10 --verbose 10 --num_ranks_in_server 1 --config_path models/vgg16/gpus=2/mp_conf.json 2>&1 | tee /home/ubuntu/pipedream/output_logs/2020-04-21T03:37:53/output.log.1; rm launch.py'"
diff --git a/archive_logs/2020-04-21T03:37:53/git_info.log b/archive_logs/2020-04-21T03:37:53/git_info.log
deleted file mode 100644
index 1965348..0000000
--- a/archive_logs/2020-04-21T03:37:53/git_info.log
+++ /dev/null
@@ -1,36 +0,0 @@
-ca7e005c7901728f212037a8fa7474f637c17ed8
-diff --git a/runtime/communication.py b/runtime/communication.py
-index ecb5b70..604ba0a 100644
---- a/runtime/communication.py
-+++ b/runtime/communication.py
-@@ -28,6 +28,7 @@ class CommunicationHandler(object):
- 
-         Note: To turn off broadcasting, set num_ranks_in_server = 1.
-         """
-+        print("Enter CommunicationHandler.__init__")
-         self.rank = rank
-         self.local_rank = local_rank
-         self.backend = backend
-@@ -39,7 +40,9 @@ class CommunicationHandler(object):
-         # Initialize the distributed environment.
-         os.environ['MASTER_ADDR'] = master_addr
-         os.environ['MASTER_PORT'] = str(master_port)
-+        print(os.environ['MASTER_ADDR'], os.environ['MASTER_PORT'])
-         dist.init_process_group(backend, rank=rank, world_size=world_size)
-+        print("Hit B")
-         assert dist.get_world_size() == self.world_size
-         print("Finished initializing process group; backend: %s, rank: %d, "
-               "world_size: %d" % (backend, rank, world_size))
-diff --git a/runtime/image_classification/driver_configs/vgg16_2mp_exp.yml b/runtime/image_classification/driver_configs/vgg16_2mp_exp.yml
-index 32ad5c1..418bcb7 100644
---- a/runtime/image_classification/driver_configs/vgg16_2mp_exp.yml
-+++ b/runtime/image_classification/driver_configs/vgg16_2mp_exp.yml
-@@ -3,7 +3,7 @@
- 'data_dir': '/home/ubuntu/data/imagenet/'
- 'config_file': 'models/vgg16/gpus=2/mp_conf.json'
- 'container': 'nvcr.io/nvidia/pytorch:19.05-py3'
--'machines': ['localhost:0', '172.31.14.150:1']
-+'machines': ['172.31.14.215:0', '172.31.14.150:1']
- 'batch_size': 64
- 'learning_rate': 0.01
- 'weight_decay': 0.0005
diff --git a/archive_logs/2020-04-21T03:37:53/gpus=2/__init__.py b/archive_logs/2020-04-21T03:37:53/gpus=2/__init__.py
deleted file mode 100644
index 0d0991d..0000000
--- a/archive_logs/2020-04-21T03:37:53/gpus=2/__init__.py
+++ /dev/null
@@ -1,19 +0,0 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT license.
-
-from .stage0 import Stage0
-from .stage1 import Stage1
-from .vgg16 import VGG16Partitioned
-
-def arch():
-    return "vgg16"
-
-def model(criterion):
-    return [
-        (Stage0(), ["input0"], ["out0"]),
-        (Stage1(), ["out0"], ["out1"]),
-        (criterion, ["out1"], ["loss"])
-    ]
-
-def full_model():
-    return VGG16Partitioned()
diff --git a/archive_logs/2020-04-21T03:37:53/gpus=2/dp_conf.json b/archive_logs/2020-04-21T03:37:53/gpus=2/dp_conf.json
deleted file mode 100644
index 863640a..0000000
--- a/archive_logs/2020-04-21T03:37:53/gpus=2/dp_conf.json
+++ /dev/null
@@ -1,6 +0,0 @@
-{
-    "module_to_stage_map": [0, 0, 0],
-    "stage_to_rank_map": {
-        "0": [0, 1]
-    }
-}
diff --git a/archive_logs/2020-04-21T03:37:53/gpus=2/hybrid_conf.json b/archive_logs/2020-04-21T03:37:53/gpus=2/hybrid_conf.json
deleted file mode 100644
index 8d3de8d..0000000
--- a/archive_logs/2020-04-21T03:37:53/gpus=2/hybrid_conf.json
+++ /dev/null
@@ -1,7 +0,0 @@
-{
-    "module_to_stage_map": [0, 1, 1],
-    "stage_to_rank_map": {
-        "0": [0, 1],
-        "1": [2]
-    }
-}
diff --git a/archive_logs/2020-04-21T03:37:53/gpus=2/mp_conf.json b/archive_logs/2020-04-21T03:37:53/gpus=2/mp_conf.json
deleted file mode 100644
index b21d1c5..0000000
--- a/archive_logs/2020-04-21T03:37:53/gpus=2/mp_conf.json
+++ /dev/null
@@ -1,7 +0,0 @@
-{
-    "module_to_stage_map": [0, 1, 1],
-    "stage_to_rank_map": {
-        "0": [0],
-        "1": [1]
-    }
-}
diff --git a/archive_logs/2020-04-21T03:37:53/gpus=2/stage0.py b/archive_logs/2020-04-21T03:37:53/gpus=2/stage0.py
deleted file mode 100644
index e6fa4a4..0000000
--- a/archive_logs/2020-04-21T03:37:53/gpus=2/stage0.py
+++ /dev/null
@@ -1,44 +0,0 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT license.
-
-import torch
-
-
-class Stage0(torch.nn.Module):
-    def __init__(self):
-        super(Stage0, self).__init__()
-        self.layer2 = torch.nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
-        self.layer3 = torch.nn.ReLU(inplace=True)
-        self.layer4 = torch.nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
-        self.layer5 = torch.nn.ReLU(inplace=True)
-        self.layer6 = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
-        self.layer7 = torch.nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
-        self.layer8 = torch.nn.ReLU(inplace=True)
-        self.layer9 = torch.nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
-
-        self._initialize_weights()
-
-    def forward(self, input0):
-        out0 = input0.clone()
-        out2 = self.layer2(out0)
-        out3 = self.layer3(out2)
-        out4 = self.layer4(out3)
-        out5 = self.layer5(out4)
-        out6 = self.layer6(out5)
-        out7 = self.layer7(out6)
-        out8 = self.layer8(out7)
-        out9 = self.layer9(out8)
-        return out9
-
-    def _initialize_weights(self):
-        for m in self.modules():
-            if isinstance(m, torch.nn.Conv2d):
-                torch.nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
-                if m.bias is not None:
-                    torch.nn.init.constant_(m.bias, 0)
-            elif isinstance(m, torch.nn.BatchNorm2d):
-                torch.nn.init.constant_(m.weight, 1)
-                torch.nn.init.constant_(m.bias, 0)
-            elif isinstance(m, torch.nn.Linear):
-                torch.nn.init.normal_(m.weight, 0, 0.01)
-                torch.nn.init.constant_(m.bias, 0)
\ No newline at end of file
diff --git a/archive_logs/2020-04-21T03:37:53/gpus=2/stage1.py b/archive_logs/2020-04-21T03:37:53/gpus=2/stage1.py
deleted file mode 100644
index af67095..0000000
--- a/archive_logs/2020-04-21T03:37:53/gpus=2/stage1.py
+++ /dev/null
@@ -1,90 +0,0 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT license.
-
-import torch
-
-
-class Stage1(torch.nn.Module):
-    def __init__(self):
-        super(Stage1, self).__init__()
-        self.layer1 = torch.nn.ReLU(inplace=True)
-        self.layer2 = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
-        self.layer3 = torch.nn.Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
-        self.layer4 = torch.nn.ReLU(inplace=True)
-        self.layer5 = torch.nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
-        self.layer6 = torch.nn.ReLU(inplace=True)
-        self.layer7 = torch.nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
-        self.layer8 = torch.nn.ReLU(inplace=True)
-        self.layer9 = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
-        self.layer10 = torch.nn.Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
-        self.layer11 = torch.nn.ReLU(inplace=True)
-        self.layer12 = torch.nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
-        self.layer13 = torch.nn.ReLU(inplace=True)
-        self.layer14 = torch.nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
-        self.layer15 = torch.nn.ReLU(inplace=True)
-        self.layer16 = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
-        self.layer17 = torch.nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
-        self.layer18 = torch.nn.ReLU(inplace=True)
-        self.layer19 = torch.nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
-        self.layer20 = torch.nn.ReLU(inplace=True)
-        self.layer21 = torch.nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
-        self.layer22 = torch.nn.ReLU(inplace=True)
-        self.layer23 = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
-        self.layer26 = torch.nn.Linear(in_features=25088, out_features=4096, bias=True)
-        self.layer27 = torch.nn.ReLU(inplace=True)
-        self.layer28 = torch.nn.Dropout(p=0.5)
-        self.layer29 = torch.nn.Linear(in_features=4096, out_features=4096, bias=True)
-        self.layer30 = torch.nn.ReLU(inplace=True)
-        self.layer31 = torch.nn.Dropout(p=0.5)
-        self.layer32 = torch.nn.Linear(in_features=4096, out_features=1000, bias=True)
-
-        self._initialize_weights()
-
-    def forward(self, input0):
-        out0 = input0.clone()
-        out1 = self.layer1(out0)
-        out2 = self.layer2(out1)
-        out3 = self.layer3(out2)
-        out4 = self.layer4(out3)
-        out5 = self.layer5(out4)
-        out6 = self.layer6(out5)
-        out7 = self.layer7(out6)
-        out8 = self.layer8(out7)
-        out9 = self.layer9(out8)
-        out10 = self.layer10(out9)
-        out11 = self.layer11(out10)
-        out12 = self.layer12(out11)
-        out13 = self.layer13(out12)
-        out14 = self.layer14(out13)
-        out15 = self.layer15(out14)
-        out16 = self.layer16(out15)
-        out17 = self.layer17(out16)
-        out18 = self.layer18(out17)
-        out19 = self.layer19(out18)
-        out20 = self.layer20(out19)
-        out21 = self.layer21(out20)
-        out22 = self.layer22(out21)
-        out23 = self.layer23(out22)
-        out24 = out23.size(0)
-        out25 = out23.view(out24, -1)
-        out26 = self.layer26(out25)
-        out27 = self.layer27(out26)
-        out28 = self.layer28(out27)
-        out29 = self.layer29(out28)
-        out30 = self.layer30(out29)
-        out31 = self.layer31(out30)
-        out32 = self.layer32(out31)
-        return out32
-
-    def _initialize_weights(self):
-        for m in self.modules():
-            if isinstance(m, torch.nn.Conv2d):
-                torch.nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
-                if m.bias is not None:
-                    torch.nn.init.constant_(m.bias, 0)
-            elif isinstance(m, torch.nn.BatchNorm2d):
-                torch.nn.init.constant_(m.weight, 1)
-                torch.nn.init.constant_(m.bias, 0)
-            elif isinstance(m, torch.nn.Linear):
-                torch.nn.init.normal_(m.weight, 0, 0.01)
-                torch.nn.init.constant_(m.bias, 0)
\ No newline at end of file
diff --git a/archive_logs/2020-04-21T03:37:53/gpus=2/vgg16.py b/archive_logs/2020-04-21T03:37:53/gpus=2/vgg16.py
deleted file mode 100644
index e23f10d..0000000
--- a/archive_logs/2020-04-21T03:37:53/gpus=2/vgg16.py
+++ /dev/null
@@ -1,17 +0,0 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT license.
-
-import torch
-from .stage0 import Stage0
-from .stage1 import Stage1
-
-class VGG16Partitioned(torch.nn.Module):
-    def __init__(self):
-        super(VGG16Partitioned, self).__init__()
-        self.stage0 = Stage0()
-        self.stage1 = Stage1()
-
-    def forward(self, input0):
-        out0 = self.stage0(input0)
-        out1 = self.stage1(out0)
-        return out1
diff --git a/archive_logs/2020-04-21T03:37:53/machinefile b/archive_logs/2020-04-21T03:37:53/machinefile
deleted file mode 100644
index cf05e7a..0000000
--- a/archive_logs/2020-04-21T03:37:53/machinefile
+++ /dev/null
@@ -1,2 +0,0 @@
-172.31.14.215
-172.31.14.150
diff --git a/archive_logs/2020-04-21T03:37:53/output.log.0 b/archive_logs/2020-04-21T03:37:53/output.log.0
deleted file mode 100644
index d678699..0000000
--- a/archive_logs/2020-04-21T03:37:53/output.log.0
+++ /dev/null
@@ -1,16 +0,0 @@
-Enter CommunicationHandler.__init__
-172.31.14.215 12345
-Hit B
-Finished initializing process group; backend: gloo, rank: 0, world_size: 2
-Send ranks:  {'out0': [1], 'target': [1]}
-Receive ranks:  {}
-Letting in 1 warm-up minibatches
-Running training for 20018 minibatches
-Forward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 0.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 411042304.000 bytes
-Epoch: 0 Step 0 	Learning rate: 0.010000
-Epoch: [0][0/20018]	Memory: 6.323 (8.313)
diff --git a/archive_logs/2020-04-21T03:37:53/vgg16_2mp_exp.yml b/archive_logs/2020-04-21T03:37:53/vgg16_2mp_exp.yml
deleted file mode 100644
index 418bcb7..0000000
--- a/archive_logs/2020-04-21T03:37:53/vgg16_2mp_exp.yml
+++ /dev/null
@@ -1,17 +0,0 @@
-'log_directory': '/home/ubuntu/pipedream/output_logs'
-'module': 'models.vgg16.gpus=2'
-'data_dir': '/home/ubuntu/data/imagenet/'
-'config_file': 'models/vgg16/gpus=2/mp_conf.json'
-'container': 'nvcr.io/nvidia/pytorch:19.05-py3'
-'machines': ['172.31.14.215:0', '172.31.14.150:1']
-'batch_size': 64
-'learning_rate': 0.01
-'weight_decay': 0.0005
-'epochs': 60
-'print_frequency': 10
-'verbose_frequency': 10
-'compress_activations': False
-'compress_in_gpu': False
-'learning_rate_policy': 'polynomial'
-'model_type': 'image_classification'
-'distributed_backend': 'gloo'
diff --git a/archive_logs/2020-04-27T00:41:40_stage0/command_history.log b/archive_logs/2020-04-27T00:41:40_stage0/command_history.log
deleted file mode 100644
index d31deec..0000000
--- a/archive_logs/2020-04-27T00:41:40_stage0/command_history.log
+++ /dev/null
@@ -1,2 +0,0 @@
-ssh -n 172.31.15.10 -o StrictHostKeyChecking=no "nvidia-docker run -d -v /home/ubuntu:/home/ubuntu -v /home/ubuntu/pipedream:/home/ubuntu/pipedream --net=host --shm-size 16g nvcr.io/nvidia/pytorch:19.05-py3 /bin/bash -c 'cp /home/ubuntu/pipedream/runtime/launch.py /home/ubuntu/pipedream/runtime/image_classification;  cd /home/ubuntu/pipedream/runtime/image_classification; python -m launch --nnodes 2 --node_rank 0 --nproc_per_node 1 main_with_runtime.py --data_dir /home/ubuntu/data/imagenet_tiny/ --master_addr 172.31.15.10 --module models.vgg16.gpus=2 --checkpoint_dir /home/ubuntu/pipedream/output_logs/2020-04-27T00:41:40 --distributed_backend gloo  -b 64 --lr 0.010000 --lr_policy polynomial --weight-decay 0.000500 --epochs 60 --print-freq 10 --verbose 10 --num_ranks_in_server 1 --config_path models/vgg16/gpus=2/mp_conf.json 2>&1 | tee /home/ubuntu/pipedream/output_logs/2020-04-27T00:41:40/output.log.0; rm launch.py'"
-ssh -n 172.31.12.252 -o StrictHostKeyChecking=no "nvidia-docker run -d -v /home/ubuntu:/home/ubuntu -v /home/ubuntu/pipedream:/home/ubuntu/pipedream --net=host --shm-size 16g nvcr.io/nvidia/pytorch:19.05-py3 /bin/bash -c 'cp /home/ubuntu/pipedream/runtime/launch.py /home/ubuntu/pipedream/runtime/image_classification;  cd /home/ubuntu/pipedream/runtime/image_classification; python -m launch --nnodes 2 --node_rank 1 --nproc_per_node 1 main_with_runtime.py --data_dir /home/ubuntu/data/imagenet_tiny/ --master_addr 172.31.15.10 --module models.vgg16.gpus=2 --checkpoint_dir /home/ubuntu/pipedream/output_logs/2020-04-27T00:41:40 --distributed_backend gloo  -b 64 --lr 0.010000 --lr_policy polynomial --weight-decay 0.000500 --epochs 60 --print-freq 10 --verbose 10 --num_ranks_in_server 1 --config_path models/vgg16/gpus=2/mp_conf.json 2>&1 | tee /home/ubuntu/pipedream/output_logs/2020-04-27T00:41:40/output.log.1; rm launch.py'"
diff --git a/archive_logs/2020-04-27T00:41:40_stage0/git_info.log b/archive_logs/2020-04-27T00:41:40_stage0/git_info.log
deleted file mode 100644
index 0cab4c9..0000000
--- a/archive_logs/2020-04-27T00:41:40_stage0/git_info.log
+++ /dev/null
@@ -1,429 +0,0 @@
-f0c5eae2f25ed9f00ed9d9064082782423441296
-diff --git a/runtime/communication.py b/runtime/communication.py
-index 18743e7..c856875 100644
---- a/runtime/communication.py
-+++ b/runtime/communication.py
-@@ -67,8 +67,6 @@ class CommunicationHandler(object):
-         assert len(self.ranks_in_server) == num_ranks_in_server - 1, \
-             self.ranks_in_server
- 
--        self.prefix = "[rank={}] ".format(self.rank)
--
-     def is_gpu_to_gpu_comm(self, connected_rank):
-         if connected_rank in self.ranks_in_server:
-             return True
-@@ -614,7 +612,7 @@ class CommunicationHandler(object):
-             self.forward_send_queues[tensor_name][index].add(tensor)
- 
-     def recv_block(self, forward_minibatch_id, backward_minibatch_id):
--        print(self.prefix + "enter recv_block", "f_mini-b_mini:", forward_minibatch_id, "-",backward_minibatch_id)
-+        print("enter recv_block", "f_mini-b_mini:", forward_minibatch_id, "-",backward_minibatch_id)
-         index = self.get_messaging_index(sending=False)
-         # block if queue empty
-         tensor_name = "out0"
-@@ -625,7 +623,7 @@ class CommunicationHandler(object):
-         return tensor
- 
-     def send_block(self, tensor, forward_minibatch_id, backward_minibatch_id):
--        print(self.prefix + "enter send_block", "f_mini-b_mini:", forward_minibatch_id, "-",backward_minibatch_id)
-+        print("enter send_block", "f_mini-b_mini:", forward_minibatch_id, "-",backward_minibatch_id)
-         tensor_name = "out0"
-         index = (forward_minibatch_id + self.rank_in_stage) % \
-                 len(self.send_ranks[tensor_name])
-diff --git a/runtime/driver.py b/runtime/driver.py
-index 3b691c7..f28b945 100644
---- a/runtime/driver.py
-+++ b/runtime/driver.py
-@@ -47,8 +47,12 @@ Remaining TODOs:
-     3) Support non-NFS checkpoint directories.
- '''
- 
-+# for remote machine
-+# rm -rf pipedream.bak
-+# mv pipedream pipedream.bak
-+# scp pipedream ~/
- 
--def create_output_folder(conf):
-+def create_output_folder(conf, workers=[]):
-     output_folder_path = os.path.join(
-         conf[LOG_DIR], datetime.datetime.now().isoformat().split('.')[0])
-     sys.stdout.write('Creating output folder: %s\n' % output_folder_path)
-@@ -57,6 +61,23 @@ def create_output_folder(conf):
-         os.makedirs(output_folder_path)
-     except OSError:
-         raise OSError
-+    
-+    # copy whole code to remote machines
-+    for rank, worker in enumerate(workers):
-+        if (rank == 0):
-+            continue
-+        print("rank:", rank, "worker:", worker)
-+
-+        shell_cmd = "rm -rf /home/ubuntu/pipedream.bak && mv /home/ubuntu/pipedream /home/ubuntu/pipedream.bak"
-+        ssh_cmd = 'ssh -n %s -o StrictHostKeyChecking=no \"%s\"' % (worker.ip, shell_cmd)
-+        subprocess.check_output(ssh_cmd, shell=True)
-+
-+        print("executed:", ssh_cmd)
-+
-+        scp_cmd = 'scp -r ~/pipedream %s:~/' % (worker.ip)
-+        subprocess.check_output(scp_cmd, shell=True)
-+
-+        print("executed:", scp_cmd)            
- 
-     return output_folder_path
- 
-@@ -155,7 +176,7 @@ if __name__ == "__main__":
-     assert len(workers) == len(configurations[MACHINES])
- 
-     # Create output directory.
--    output_dir = create_output_folder(conf=configurations)
-+    output_dir = create_output_folder(conf=configurations, workers=workers)
- 
-     # Copy configuration file to output folder.
-     copy_command = 'cp %s %s' % (args.config_file, output_dir)
-diff --git a/runtime/image_classification/driver_configs/vgg16_2mp_exp.yml b/runtime/image_classification/driver_configs/vgg16_2mp_exp.yml
-index 32ad5c1..4d9ce0b 100644
---- a/runtime/image_classification/driver_configs/vgg16_2mp_exp.yml
-+++ b/runtime/image_classification/driver_configs/vgg16_2mp_exp.yml
-@@ -1,9 +1,9 @@
- 'log_directory': '/home/ubuntu/pipedream/output_logs'
- 'module': 'models.vgg16.gpus=2'
--'data_dir': '/home/ubuntu/data/imagenet/'
-+'data_dir': '/home/ubuntu/data/imagenet_tiny/'
- 'config_file': 'models/vgg16/gpus=2/mp_conf.json'
- 'container': 'nvcr.io/nvidia/pytorch:19.05-py3'
--'machines': ['localhost:0', '172.31.14.150:1']
-+'machines': ['172.31.15.10:0', '172.31.12.252:1']
- 'batch_size': 64
- 'learning_rate': 0.01
- 'weight_decay': 0.0005
-diff --git a/runtime/image_classification/main_with_runtime.py b/runtime/image_classification/main_with_runtime.py
-index 08ee54e..fa91289 100644
---- a/runtime/image_classification/main_with_runtime.py
-+++ b/runtime/image_classification/main_with_runtime.py
-@@ -135,31 +135,36 @@ def main():
-         input_size = [args.batch_size, 3, 299, 299]
-     else:
-         input_size = [args.batch_size, 3, 224, 224]
--    training_tensor_shapes = {"input0": input_size, "target": [args.batch_size]}
--    dtypes = {"input0": torch.int64, "target": torch.int64}
-+    # training_tensor_shapes = {"input0": input_size, "target": [args.batch_size]}
-+    # dtypes = {"input0": torch.int64, "target": torch.int64}
-     inputs_module_destinations = {"input": 0}
-     target_tensor_names = {"target"}
-+    
-+    training_tensor_shapes = {'input0': (64, 3, 224, 224), 'target': (64,), 'out0': (64, 128, 112, 112), 'out1': (64, 1000)}
-+    eval_tensor_shapes = {'input0': (100, 3, 224, 224), 'target': (100,), 'out0': (100, 128, 112, 112), 'out1': (100, 1000)}
-+    dtypes = {'input0': torch.int64, 'target': torch.int64, 'out0': torch.float32, 'out1': torch.float32}
-+
-     for (stage, inputs, outputs) in model[:-1]:  # Skip last layer (loss).
-         input_tensors = []
-         for input in inputs:
-             input_tensor = torch.zeros(tuple(training_tensor_shapes[input]),
-                                        dtype=torch.float32)
-             input_tensors.append(input_tensor)
--        with torch.no_grad():
--            output_tensors = stage(*tuple(input_tensors))
--        if not type(output_tensors) is tuple:
--            output_tensors = [output_tensors]
--        for output, output_tensor in zip(outputs,
--                                         list(output_tensors)):
--            training_tensor_shapes[output] = list(output_tensor.size())
--            dtypes[output] = output_tensor.dtype
--
--    eval_tensor_shapes = {}
--    for key in training_tensor_shapes:
--        eval_tensor_shapes[key] = tuple(
--            [args.eval_batch_size] + training_tensor_shapes[key][1:])
--        training_tensor_shapes[key] = tuple(
--            training_tensor_shapes[key])
-+        # with torch.no_grad():
-+        #     output_tensors = stage(*tuple(input_tensors))
-+        # if not type(output_tensors) is tuple:
-+        #     output_tensors = [output_tensors]
-+        # for output, output_tensor in zip(outputs,
-+        #                                  list(output_tensors)):
-+        #     training_tensor_shapes[output] = list(output_tensor.size())
-+        #     dtypes[output] = output_tensor.dtype
-+
-+    # eval_tensor_shapes = {}
-+    # for key in training_tensor_shapes:
-+    #     eval_tensor_shapes[key] = tuple(
-+    #         [args.eval_batch_size] + training_tensor_shapes[key][1:])
-+    #     training_tensor_shapes[key] = tuple(
-+    #         training_tensor_shapes[key])
- 
-     configuration_maps = {
-         'module_to_stage_map': None,
-diff --git a/runtime/image_classification/models/vgg16/gpus=2/stage0.py b/runtime/image_classification/models/vgg16/gpus=2/stage0.py
-index 454081e..3111298 100644
---- a/runtime/image_classification/models/vgg16/gpus=2/stage0.py
-+++ b/runtime/image_classification/models/vgg16/gpus=2/stage0.py
-@@ -27,8 +27,8 @@ class Stage0(torch.nn.Module):
-         out6 = self.layer6(out5)
-         out7 = self.layer7(out6)
-         out8 = self.layer8(out7)
--        self.upstream_tail(out8, forward_minibatch_id, backward_minibatch_id, comm_handler)
--        return None
-+        out9 = self.upstream_tail(out8, forward_minibatch_id, backward_minibatch_id, comm_handler)
-+        return out9
- 
-     def _initialize_weights(self):
-         for m in self.modules():
-@@ -59,6 +59,7 @@ class Upstream_Tail(torch.nn.Module):
-     def forward(self, inp, forward_minibatch_id, backward_minibatch_id, comm_handler):
-         print("Start upstream tail layer")
-         
-+        block_out_list = []
-         block_num = 4
-         
-         batch_size, c_in = inp.shape[0], inp.shape[1]
-@@ -75,6 +76,7 @@ class Upstream_Tail(torch.nn.Module):
-         block_inp = inp[:, :, h_start:h_end, w_start:w_end]
-         
-         block_out = self.conv2d(block_inp)
-+        block_out_list.append(block_out)
-         
-         comm_handler.send_block(block_out, forward_minibatch_id=forward_minibatch_id,
-                                      backward_minibatch_id=backward_minibatch_id)
-@@ -88,6 +90,7 @@ class Upstream_Tail(torch.nn.Module):
-         block_inp = inp[:, :, h_start:h_end, w_start:w_end]
- 
-         block_out = self.conv2d(block_inp)
-+        block_out_list.append(block_out)
- 
-         comm_handler.send_block(block_out, forward_minibatch_id=forward_minibatch_id,
-                                      backward_minibatch_id=backward_minibatch_id)
-@@ -101,6 +104,7 @@ class Upstream_Tail(torch.nn.Module):
-         block_inp = inp[:, :, h_start:h_end, w_start:w_end]
- 
-         block_out = self.conv2d(block_inp)
-+        block_out_list.append(block_out)
- 
-         comm_handler.send_block(block_out, forward_minibatch_id=forward_minibatch_id,
-                              backward_minibatch_id=backward_minibatch_id)
-@@ -114,10 +118,18 @@ class Upstream_Tail(torch.nn.Module):
-         block_inp = inp[:, :, h_start:h_end, w_start:w_end]
- 
-         block_out = self.conv2d(block_inp)
-+        block_out_list.append(block_out)
- 
-         comm_handler.send_block(block_out, forward_minibatch_id=forward_minibatch_id,
-                                      backward_minibatch_id=backward_minibatch_id)
- 
-         print("block3:", "inp:", block_inp.shape, "out:", block_out.shape)
- 
--        return None
-\ No newline at end of file
-+        return self._combine(block_out_list)
-+    
-+    def _combine(self, block_list):
-+        block_upper = torch.cat((block_list[0], block_list[1]), dim=3)
-+        block_lower = torch.cat((block_list[2], block_list[3]), dim=3)
-+        combined_inp = torch.cat((block_upper, block_lower), dim=2)
-+
-+        return combined_inp  
-\ No newline at end of file
-diff --git a/runtime/image_classification/models/vgg16/gpus=2/stage1.py b/runtime/image_classification/models/vgg16/gpus=2/stage1.py
-index e06f4c9..29e8ea6 100644
---- a/runtime/image_classification/models/vgg16/gpus=2/stage1.py
-+++ b/runtime/image_classification/models/vgg16/gpus=2/stage1.py
-@@ -40,9 +40,8 @@ class Stage1(torch.nn.Module):
- 
-         self._initialize_weights()
- 
--    def forward(self, forward_minibatch_id, backward_minibatch_id, comm_handler):
--        out0 = self.downstream_head(forward_minibatch_id, backward_minibatch_id, comm_handler)
--        out1 = self.layer1(out0)
-+    def forward(self, forward_minibatch_id, backward_minibatch_id, r):
-+        out1 = self.downstream_head(forward_minibatch_id, backward_minibatch_id, r)
-         out2 = self.layer2(out1)
-         out3 = self.layer3(out2)
-         out4 = self.layer4(out3)
-@@ -95,22 +94,41 @@ class Downstream_Head(torch.nn.Module):
-         print("initialize downstream head module")
-         self.relu = torch.nn.ReLU(inplace=inplace)
-              
--    def forward(self, forward_minibatch_id, backward_minibatch_id, comm_handler):
-+    def forward(self, forward_minibatch_id, backward_minibatch_id, r):
-         print("Start downstream head layer")
- 
-         block_num = 4
-+        block_buffer = torch.zeros(64, 128, 112, 112).to("cuda")
-         block_out_relu = []
-         
-         for block_id in range(block_num):
--            block_inp_relu = self.comm_handler.recv_block(forward_minibatch_id, backward_minibatch_id)
--            block_out_relu.append(self.relu(block_inp_relu))
-+            block_inp_relu = r.comm_handler.recv_block(forward_minibatch_id, backward_minibatch_id)
-+            print("recv: tensor:", block_inp_relu.shape)
-+            # store block_inp_relu into buffer
-+            # slice and clone buffer and pass into ReLU
-+            # return buffer as input_tensor
-+            if (block_id == 0):
-+                block_buffer[:, :, :57, :57] = block_inp_relu
-+                block_out_relu.append(self.relu(block_buffer[:, :, :57, :57].clone()))
-+            elif (block_id == 1):
-+                block_buffer[:, :, :57, 57:] = block_inp_relu
-+                block_out_relu.append(self.relu(block_buffer[:, :, :57, 57:].clone()))
-+            elif(block_id == 2):
-+                block_buffer[:, :, 57:, :57] = block_inp_relu
-+                block_out_relu.append(self.relu(block_buffer[:, :, 57:, :57].clone()))
-+            else:
-+                block_buffer[:, :, 57:, 57:] = block_inp_relu
-+                block_out_relu.append(self.relu(block_buffer[:, :, 57:, 57:].clone()))
- 
--            # Used to track where to receive forward from.
--            comm_handler.increment_messaging_index(
--                sending=False)
-+
-+        # Used to track where to receive forward from.
-+        r.comm_handler.increment_messaging_index(
-+            sending=False)
-         
-         relu_out = self._combine(block_out_relu)
- 
-+        r.tensors[-1]["out0"] = block_buffer
-+
-         return relu_out
- 
-     def _combine(self, block_list):
-diff --git a/runtime/runtime_block.py b/runtime/runtime_block.py
-index 6f5b0db..3e26f06 100644
---- a/runtime/runtime_block.py
-+++ b/runtime/runtime_block.py
-@@ -75,8 +75,6 @@ class StageRuntime:
-         # computed from the forward pass for the backward pass.
-         self.enable_recompute = enable_recompute
- 
--        self.prefix = "[stage={}] ".format(self.stage)
--
-         # Disable recomputation for the last stage.
-         if rank == num_ranks_in_server - 1:
-             self.enable_recompute = False
-@@ -425,24 +423,25 @@ class StageRuntime:
-                     non_blocking=True)
-         # Other Stages (there is no dataloader)
-         else:
--            print(self.prefix + "pass receive_tensors_forward")
--            pass
--            # # Receive all required tensors from upstream GPU.
--            # # TODO: why recv does not need rank?
--            # for input_name in self.receive_ranks:
--            #     if input_name == "ack":
--            #         continue
--
--            #     self.tensors[-1][input_name] = \
--            #         self.comm_handler.recv(
--            #             input_name,
--            #             forward_minibatch_id=self.forward_minibatch_id,
--            #             backward_minibatch_id=self.backward_minibatch_id,
--            #             backward=False)
--
--            #     self.forward_stats.stats['receive_tensors_size'] += \
--            #         (self.tensors[-1][input_name].element_size() *
--            #          self.tensors[-1][input_name].nelement())
-+            # Receive all required tensors from upstream GPU.
-+            # TODO: why recv does not need rank?
-+            for input_name in self.receive_ranks:
-+                if input_name == "ack" or input_name == "out0":
-+                    print("skip recv:", input_name)
-+                    continue
-+
-+                print("try recv:", input_name)
-+                self.tensors[-1][input_name] = \
-+                    self.comm_handler.recv(
-+                        input_name,
-+                        forward_minibatch_id=self.forward_minibatch_id,
-+                        backward_minibatch_id=self.backward_minibatch_id,
-+                        backward=False)
-+
-+                print("recv:", input_name)
-+                self.forward_stats.stats['receive_tensors_size'] += \
-+                    (self.tensors[-1][input_name].element_size() *
-+                     self.tensors[-1][input_name].nelement())
- 
-             # # Used to track where to receive forward from.
-             # self.comm_handler.increment_messaging_index(
-@@ -451,9 +450,11 @@ class StageRuntime:
-     def send_tensors_forward(self):
-         # Send all required tensors downstream.
-         for output_name in self.send_ranks:
--            if output_name == "ack":
-+            if output_name == "ack" or output_name == "out0":
-+                print("skip send:", output_name)
-                 continue
- 
-+            print("try send:", output_name)
-             self.comm_handler.send(
-                 output_name,
-                 self.tensors[-1][output_name],
-@@ -461,6 +462,7 @@ class StageRuntime:
-                 backward_minibatch_id=self.backward_minibatch_id,
-                 backward=False)
- 
-+            print("sent:", output_name)
-             self.forward_stats.stats['send_tensors_size'] += \
-                 (self.tensors[-1][output_name].element_size() *
-                  self.tensors[-1][output_name].nelement())
-@@ -509,7 +511,7 @@ class StageRuntime:
-     def run_forward(self, recompute_step=False):
-         """Run forward pass.
-         """
--        print(self.prefix + "enter run_forward")
-+        print("enter run_forward")
-         # Receive tensors from previous worker.
-         self.receive_tensors_forward()
-         tensors = self.tensors[-1]
-@@ -518,14 +520,14 @@ class StageRuntime:
-         self._run_forward(tensors)
- 
-         # Send tensors forward.
--        # self.send_tensors_forward()
-+        self.send_tensors_forward()
-         # if self.verbose_freq > 0 and self.forward_minibatch_id % self.verbose_freq == 0:
-         #     self.forward_stats.print_stats()
-         # self.forward_stats.reset_stats()
-         self.forward_minibatch_id += 1
- 
-     def _run_forward(self, tensors):
--        print(self.prefix + "enter _run_forward")
-+        print("enter _run_forward")
-         # Perform forward pass through model (self.modules_with_dependencies already
-         # has modules in topological order).
-         modules = self.modules_with_dependencies.modules()
-@@ -549,12 +551,11 @@ class StageRuntime:
-                     module_outputs = [sum(module_outputs)]
-             else:
-                 # If layer is non-criterion.
--                module_outputs = module(*[tensors[input_name]
--                                          for input_name in input_names])
-+                # module_outputs = module(*[tensors[input_name]
-+                #                           for input_name in input_names])
-                 
-                 # stage 0
-                 if self.loader_iter is not None:
--                    print(self.prefix + "pass tensor into module: " + module)
-                     module_outputs = module(*[tensors[input_name]
-                                           for input_name in input_names],
-                                           forward_minibatch_id=self.forward_minibatch_id, 
-@@ -562,17 +563,15 @@ class StageRuntime:
-                                           comm_handler=self.comm_handler)
-                 # other stages
-                 else:
--                    print(self.prefix + "enter into module: " + module)
-                     module_outputs = module(forward_minibatch_id=self.forward_minibatch_id, 
-                                           backward_minibatch_id=self.backward_minibatch_id, 
--                                          comm_handler=self.comm_handler)
-+                                          r=self)
-                 
-                 if not isinstance(module_outputs, tuple):
-                     module_outputs = (module_outputs,)
-                 module_outputs = list(module_outputs)
- 
--            # TODO: may lead to error
--            print(self.prefix + "output_names:", output_names, "module_outputs: ", module_outputs)
-+
-             for (output_name, module_output) in zip(output_names, module_outputs):
-                 tensors[output_name] = module_output
- 
diff --git a/archive_logs/2020-04-27T00:41:40_stage0/gpus=2/__init__.py b/archive_logs/2020-04-27T00:41:40_stage0/gpus=2/__init__.py
deleted file mode 100644
index 2c14c75..0000000
--- a/archive_logs/2020-04-27T00:41:40_stage0/gpus=2/__init__.py
+++ /dev/null
@@ -1,19 +0,0 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT license.
-
-from .stage0 import Stage0
-from .stage1 import Stage1
-from .vgg16 import VGG16Partitioned
-
-def arch():
-    return "vgg16"
-
-def model(criterion):
-    return [
-        (Stage0(), ["input0"], ["out0"]), # stage 0
-        (Stage1(), ["out0"], ["out1"]), # stage 1
-        (criterion, ["out1"], ["loss"]) # stage 1
-    ]
-
-def full_model():
-    return VGG16Partitioned()
diff --git a/archive_logs/2020-04-27T00:41:40_stage0/gpus=2/dp_conf.json b/archive_logs/2020-04-27T00:41:40_stage0/gpus=2/dp_conf.json
deleted file mode 100644
index 863640a..0000000
--- a/archive_logs/2020-04-27T00:41:40_stage0/gpus=2/dp_conf.json
+++ /dev/null
@@ -1,6 +0,0 @@
-{
-    "module_to_stage_map": [0, 0, 0],
-    "stage_to_rank_map": {
-        "0": [0, 1]
-    }
-}
diff --git a/archive_logs/2020-04-27T00:41:40_stage0/gpus=2/hybrid_conf.json b/archive_logs/2020-04-27T00:41:40_stage0/gpus=2/hybrid_conf.json
deleted file mode 100644
index 8d3de8d..0000000
--- a/archive_logs/2020-04-27T00:41:40_stage0/gpus=2/hybrid_conf.json
+++ /dev/null
@@ -1,7 +0,0 @@
-{
-    "module_to_stage_map": [0, 1, 1],
-    "stage_to_rank_map": {
-        "0": [0, 1],
-        "1": [2]
-    }
-}
diff --git a/archive_logs/2020-04-27T00:41:40_stage0/gpus=2/mp_conf.json b/archive_logs/2020-04-27T00:41:40_stage0/gpus=2/mp_conf.json
deleted file mode 100644
index b21d1c5..0000000
--- a/archive_logs/2020-04-27T00:41:40_stage0/gpus=2/mp_conf.json
+++ /dev/null
@@ -1,7 +0,0 @@
-{
-    "module_to_stage_map": [0, 1, 1],
-    "stage_to_rank_map": {
-        "0": [0],
-        "1": [1]
-    }
-}
diff --git a/archive_logs/2020-04-27T00:41:40_stage0/gpus=2/stage0.py b/archive_logs/2020-04-27T00:41:40_stage0/gpus=2/stage0.py
deleted file mode 100644
index 3111298..0000000
--- a/archive_logs/2020-04-27T00:41:40_stage0/gpus=2/stage0.py
+++ /dev/null
@@ -1,135 +0,0 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT license.
-
-import torch
-
-
-class Stage0(torch.nn.Module):
-    def __init__(self):
-        super(Stage0, self).__init__()
-        self.layer2 = torch.nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
-        self.layer3 = torch.nn.ReLU(inplace=True)
-        self.layer4 = torch.nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
-        self.layer5 = torch.nn.ReLU(inplace=True)
-        self.layer6 = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
-        self.layer7 = torch.nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
-        self.layer8 = torch.nn.ReLU(inplace=True)
-        self.upstream_tail = Upstream_Tail(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
-
-        self._initialize_weights()
-
-    def forward(self, input0, forward_minibatch_id, backward_minibatch_id, comm_handler):
-        out0 = input0.clone()
-        out2 = self.layer2(out0)
-        out3 = self.layer3(out2)
-        out4 = self.layer4(out3)
-        out5 = self.layer5(out4)
-        out6 = self.layer6(out5)
-        out7 = self.layer7(out6)
-        out8 = self.layer8(out7)
-        out9 = self.upstream_tail(out8, forward_minibatch_id, backward_minibatch_id, comm_handler)
-        return out9
-
-    def _initialize_weights(self):
-        for m in self.modules():
-            if isinstance(m, torch.nn.Conv2d):
-                torch.nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
-                if m.bias is not None:
-                    torch.nn.init.constant_(m.bias, 0)
-            elif isinstance(m, torch.nn.BatchNorm2d):
-                torch.nn.init.constant_(m.weight, 1)
-                torch.nn.init.constant_(m.bias, 0)
-            elif isinstance(m, torch.nn.Linear):
-                torch.nn.init.normal_(m.weight, 0, 0.01)
-                torch.nn.init.constant_(m.bias, 0)
-
-class Upstream_Tail(torch.nn.Module):
-    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):
-        super(Upstream_Tail, self).__init__()
-        print("initialize upstream tail module")
-        self.orig_padding = padding
-        self.kernel_size = kernel_size[0]
-        self.conv2d = torch.nn.Conv2d(in_channels=in_channels, 
-                                      out_channels=out_channels, 
-                                      kernel_size=kernel_size,
-                                      stride=stride)
-
-        self.padder = torch.nn.ZeroPad2d(padding[0])
-    
-    def forward(self, inp, forward_minibatch_id, backward_minibatch_id, comm_handler):
-        print("Start upstream tail layer")
-        
-        block_out_list = []
-        block_num = 4
-        
-        batch_size, c_in = inp.shape[0], inp.shape[1]
-        h_i, w_i = inp.shape[2], inp.shape[3]
-        
-        inp = self.padder(inp)
-        h_pad, w_pad = inp.size(2), inp.size(3)
-        block_height, block_width = h_pad // 2,  w_pad // 2
-        
-        # block_0
-        h_start, h_end = 0, block_height + self.kernel_size-1
-        w_start, w_end = 0, block_width + self.kernel_size-1
-
-        block_inp = inp[:, :, h_start:h_end, w_start:w_end]
-        
-        block_out = self.conv2d(block_inp)
-        block_out_list.append(block_out)
-        
-        comm_handler.send_block(block_out, forward_minibatch_id=forward_minibatch_id,
-                                     backward_minibatch_id=backward_minibatch_id)
-
-        print("block0:", "inp:", block_inp.shape, "out:", block_out.shape)
-
-        # block_1
-        h_start, h_end = 0, block_height + self.kernel_size-1
-        w_start, w_end = block_width, w_pad
-
-        block_inp = inp[:, :, h_start:h_end, w_start:w_end]
-
-        block_out = self.conv2d(block_inp)
-        block_out_list.append(block_out)
-
-        comm_handler.send_block(block_out, forward_minibatch_id=forward_minibatch_id,
-                                     backward_minibatch_id=backward_minibatch_id)
-
-        print("block1:", "inp:", block_inp.shape, "out:", block_out.shape)
-
-        # block_2
-        h_start, h_end = block_height, h_pad
-        w_start, w_end = 0, block_width + self.kernel_size-1
-
-        block_inp = inp[:, :, h_start:h_end, w_start:w_end]
-
-        block_out = self.conv2d(block_inp)
-        block_out_list.append(block_out)
-
-        comm_handler.send_block(block_out, forward_minibatch_id=forward_minibatch_id,
-                             backward_minibatch_id=backward_minibatch_id)
-        
-        print("block2:", "inp:", block_inp.shape, "out:", block_out.shape)
-
-        # block_3
-        h_start, h_end = block_height, h_pad
-        w_start, w_end = block_width, w_pad
-
-        block_inp = inp[:, :, h_start:h_end, w_start:w_end]
-
-        block_out = self.conv2d(block_inp)
-        block_out_list.append(block_out)
-
-        comm_handler.send_block(block_out, forward_minibatch_id=forward_minibatch_id,
-                                     backward_minibatch_id=backward_minibatch_id)
-
-        print("block3:", "inp:", block_inp.shape, "out:", block_out.shape)
-
-        return self._combine(block_out_list)
-    
-    def _combine(self, block_list):
-        block_upper = torch.cat((block_list[0], block_list[1]), dim=3)
-        block_lower = torch.cat((block_list[2], block_list[3]), dim=3)
-        combined_inp = torch.cat((block_upper, block_lower), dim=2)
-
-        return combined_inp  
\ No newline at end of file
diff --git a/archive_logs/2020-04-27T00:41:40_stage0/gpus=2/stage1.py b/archive_logs/2020-04-27T00:41:40_stage0/gpus=2/stage1.py
deleted file mode 100644
index 29e8ea6..0000000
--- a/archive_logs/2020-04-27T00:41:40_stage0/gpus=2/stage1.py
+++ /dev/null
@@ -1,139 +0,0 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT license.
-
-import torch
-
-
-class Stage1(torch.nn.Module):
-    def __init__(self):
-        super(Stage1, self).__init__()
-        self.downstream_head = Downstream_Head(inplace=True)
-        self.layer2 = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
-        self.layer3 = torch.nn.Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
-        self.layer4 = torch.nn.ReLU(inplace=True)
-        self.layer5 = torch.nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
-        self.layer6 = torch.nn.ReLU(inplace=True)
-        self.layer7 = torch.nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
-        self.layer8 = torch.nn.ReLU(inplace=True)
-        self.layer9 = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
-        self.layer10 = torch.nn.Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
-        self.layer11 = torch.nn.ReLU(inplace=True)
-        self.layer12 = torch.nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
-        self.layer13 = torch.nn.ReLU(inplace=True)
-        self.layer14 = torch.nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
-        self.layer15 = torch.nn.ReLU(inplace=True)
-        self.layer16 = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
-        self.layer17 = torch.nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
-        self.layer18 = torch.nn.ReLU(inplace=True)
-        self.layer19 = torch.nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
-        self.layer20 = torch.nn.ReLU(inplace=True)
-        self.layer21 = torch.nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
-        self.layer22 = torch.nn.ReLU(inplace=True)
-        self.layer23 = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
-        self.layer26 = torch.nn.Linear(in_features=25088, out_features=4096, bias=True)
-        self.layer27 = torch.nn.ReLU(inplace=True)
-        self.layer28 = torch.nn.Dropout(p=0.5)
-        self.layer29 = torch.nn.Linear(in_features=4096, out_features=4096, bias=True)
-        self.layer30 = torch.nn.ReLU(inplace=True)
-        self.layer31 = torch.nn.Dropout(p=0.5)
-        self.layer32 = torch.nn.Linear(in_features=4096, out_features=1000, bias=True)
-
-        self._initialize_weights()
-
-    def forward(self, forward_minibatch_id, backward_minibatch_id, r):
-        out1 = self.downstream_head(forward_minibatch_id, backward_minibatch_id, r)
-        out2 = self.layer2(out1)
-        out3 = self.layer3(out2)
-        out4 = self.layer4(out3)
-        out5 = self.layer5(out4)
-        out6 = self.layer6(out5)
-        out7 = self.layer7(out6)
-        out8 = self.layer8(out7)
-        out9 = self.layer9(out8)
-        out10 = self.layer10(out9)
-        out11 = self.layer11(out10)
-        out12 = self.layer12(out11)
-        out13 = self.layer13(out12)
-        out14 = self.layer14(out13)
-        out15 = self.layer15(out14)
-        out16 = self.layer16(out15)
-        out17 = self.layer17(out16)
-        out18 = self.layer18(out17)
-        out19 = self.layer19(out18)
-        out20 = self.layer20(out19)
-        out21 = self.layer21(out20)
-        out22 = self.layer22(out21)
-        out23 = self.layer23(out22)
-        out24 = out23.size(0)
-        out25 = out23.view(out24, -1)
-        out26 = self.layer26(out25)
-        out27 = self.layer27(out26)
-        out28 = self.layer28(out27)
-        out29 = self.layer29(out28)
-        out30 = self.layer30(out29)
-        out31 = self.layer31(out30)
-        out32 = self.layer32(out31)
-        return out32
-
-    def _initialize_weights(self):
-        for m in self.modules():
-            if isinstance(m, torch.nn.Conv2d):
-                torch.nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
-                if m.bias is not None:
-                    torch.nn.init.constant_(m.bias, 0)
-            elif isinstance(m, torch.nn.BatchNorm2d):
-                torch.nn.init.constant_(m.weight, 1)
-                torch.nn.init.constant_(m.bias, 0)
-            elif isinstance(m, torch.nn.Linear):
-                torch.nn.init.normal_(m.weight, 0, 0.01)
-                torch.nn.init.constant_(m.bias, 0)
-
-class Downstream_Head(torch.nn.Module):
-    def __init__(self, inplace):
-        super(Downstream_Head, self).__init__()
-        print("initialize downstream head module")
-        self.relu = torch.nn.ReLU(inplace=inplace)
-             
-    def forward(self, forward_minibatch_id, backward_minibatch_id, r):
-        print("Start downstream head layer")
-
-        block_num = 4
-        block_buffer = torch.zeros(64, 128, 112, 112).to("cuda")
-        block_out_relu = []
-        
-        for block_id in range(block_num):
-            block_inp_relu = r.comm_handler.recv_block(forward_minibatch_id, backward_minibatch_id)
-            print("recv: tensor:", block_inp_relu.shape)
-            # store block_inp_relu into buffer
-            # slice and clone buffer and pass into ReLU
-            # return buffer as input_tensor
-            if (block_id == 0):
-                block_buffer[:, :, :57, :57] = block_inp_relu
-                block_out_relu.append(self.relu(block_buffer[:, :, :57, :57].clone()))
-            elif (block_id == 1):
-                block_buffer[:, :, :57, 57:] = block_inp_relu
-                block_out_relu.append(self.relu(block_buffer[:, :, :57, 57:].clone()))
-            elif(block_id == 2):
-                block_buffer[:, :, 57:, :57] = block_inp_relu
-                block_out_relu.append(self.relu(block_buffer[:, :, 57:, :57].clone()))
-            else:
-                block_buffer[:, :, 57:, 57:] = block_inp_relu
-                block_out_relu.append(self.relu(block_buffer[:, :, 57:, 57:].clone()))
-
-
-        # Used to track where to receive forward from.
-        r.comm_handler.increment_messaging_index(
-            sending=False)
-        
-        relu_out = self._combine(block_out_relu)
-
-        r.tensors[-1]["out0"] = block_buffer
-
-        return relu_out
-
-    def _combine(self, block_list):
-        block_upper = torch.cat((block_list[0], block_list[1]), dim=3)
-        block_lower = torch.cat((block_list[2], block_list[3]), dim=3)
-        combined_inp = torch.cat((block_upper, block_lower), dim=2)
-
-        return combined_inp  
\ No newline at end of file
diff --git a/archive_logs/2020-04-27T00:41:40_stage0/gpus=2/vgg16.py b/archive_logs/2020-04-27T00:41:40_stage0/gpus=2/vgg16.py
deleted file mode 100644
index e23f10d..0000000
--- a/archive_logs/2020-04-27T00:41:40_stage0/gpus=2/vgg16.py
+++ /dev/null
@@ -1,17 +0,0 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT license.
-
-import torch
-from .stage0 import Stage0
-from .stage1 import Stage1
-
-class VGG16Partitioned(torch.nn.Module):
-    def __init__(self):
-        super(VGG16Partitioned, self).__init__()
-        self.stage0 = Stage0()
-        self.stage1 = Stage1()
-
-    def forward(self, input0):
-        out0 = self.stage0(input0)
-        out1 = self.stage1(out0)
-        return out1
diff --git a/archive_logs/2020-04-27T00:41:40_stage0/machinefile b/archive_logs/2020-04-27T00:41:40_stage0/machinefile
deleted file mode 100644
index 30a42f4..0000000
--- a/archive_logs/2020-04-27T00:41:40_stage0/machinefile
+++ /dev/null
@@ -1,2 +0,0 @@
-172.31.15.10
-172.31.12.252
diff --git a/archive_logs/2020-04-27T00:41:40_stage0/output.log.0 b/archive_logs/2020-04-27T00:41:40_stage0/output.log.0
deleted file mode 100644
index 8638804..0000000
--- a/archive_logs/2020-04-27T00:41:40_stage0/output.log.0
+++ /dev/null
@@ -1,5812 +0,0 @@
-initialize upstream tail module
-initialize downstream head module
-Finished initializing process group; backend: gloo, rank: 0, world_size: 2
-Send ranks:  {'out0': [1], 'target': [1]}
-Receive ranks:  {}
-Letting in 1 warm-up minibatches
-Running training for 1562 minibatches
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 0 - 0
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 0 - 0
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 0 - 0
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 0 - 0
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 1 - 0
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 1 - 0
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 1 - 0
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 1 - 0
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-Epoch: 0 Step 0 	Learning rate: 0.010000
-Epoch: [0][0/1562]	Memory: 7.715 (8.879)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 411041792.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 0.000 bytes
-Optimizer step took: 0.001
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 2 - 1
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 2 - 1
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 2 - 1
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 2 - 1
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 3 - 2
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 3 - 2
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 3 - 2
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 3 - 2
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 4 - 3
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 4 - 3
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 4 - 3
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 4 - 3
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 5 - 4
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 5 - 4
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 5 - 4
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 5 - 4
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 6 - 5
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 6 - 5
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 6 - 5
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 6 - 5
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 7 - 6
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 7 - 6
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 7 - 6
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 7 - 6
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 8 - 7
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 8 - 7
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 8 - 7
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 8 - 7
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 9 - 8
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 9 - 8
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 9 - 8
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 9 - 8
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 10 - 9
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 10 - 9
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 10 - 9
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 10 - 9
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 11 - 10
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 11 - 10
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 11 - 10
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 11 - 10
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-Epoch: [0][10/1562]	Memory: 8.167 (12.537)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 411041792.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 0.000 bytes
-Optimizer step took: 0.001
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 12 - 11
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 12 - 11
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 12 - 11
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 12 - 11
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 13 - 12
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 13 - 12
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 13 - 12
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 13 - 12
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 14 - 13
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 14 - 13
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 14 - 13
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 14 - 13
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 15 - 14
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 15 - 14
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 15 - 14
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 15 - 14
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 16 - 15
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 16 - 15
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 16 - 15
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 16 - 15
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 17 - 16
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 17 - 16
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 17 - 16
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 17 - 16
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 18 - 17
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 18 - 17
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 18 - 17
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 18 - 17
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 19 - 18
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 19 - 18
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 19 - 18
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 19 - 18
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 20 - 19
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 20 - 19
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 20 - 19
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 20 - 19
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 21 - 20
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 21 - 20
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 21 - 20
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 21 - 20
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-Epoch: [0][20/1562]	Memory: 8.270 (12.537)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 411041792.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 0.000 bytes
-Optimizer step took: 0.442
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 22 - 21
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 22 - 21
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 22 - 21
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 22 - 21
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 23 - 22
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 23 - 22
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 23 - 22
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 23 - 22
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 24 - 23
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 24 - 23
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 24 - 23
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 24 - 23
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 25 - 24
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 25 - 24
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 25 - 24
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 25 - 24
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 26 - 25
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 26 - 25
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 26 - 25
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 26 - 25
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 27 - 26
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 27 - 26
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 27 - 26
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 27 - 26
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 28 - 27
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 28 - 27
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 28 - 27
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 28 - 27
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 29 - 28
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 29 - 28
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 29 - 28
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 29 - 28
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 30 - 29
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 30 - 29
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 30 - 29
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 30 - 29
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 31 - 30
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 31 - 30
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 31 - 30
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 31 - 30
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-Epoch: [0][30/1562]	Memory: 8.269 (12.537)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 411041792.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 0.000 bytes
-Optimizer step took: 0.001
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 32 - 31
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 32 - 31
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 32 - 31
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 32 - 31
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 33 - 32
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 33 - 32
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 33 - 32
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 33 - 32
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 34 - 33
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 34 - 33
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 34 - 33
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 34 - 33
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 35 - 34
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 35 - 34
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 35 - 34
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 35 - 34
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 36 - 35
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 36 - 35
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 36 - 35
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 36 - 35
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 37 - 36
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 37 - 36
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 37 - 36
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 37 - 36
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 38 - 37
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 38 - 37
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 38 - 37
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 38 - 37
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 39 - 38
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 39 - 38
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 39 - 38
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 39 - 38
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 40 - 39
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 40 - 39
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 40 - 39
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 40 - 39
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 41 - 40
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 41 - 40
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 41 - 40
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 41 - 40
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-Epoch: [0][40/1562]	Memory: 8.270 (12.537)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 411041792.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 0.000 bytes
-Optimizer step took: 0.001
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 42 - 41
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 42 - 41
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 42 - 41
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 42 - 41
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 43 - 42
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 43 - 42
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 43 - 42
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 43 - 42
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 44 - 43
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 44 - 43
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 44 - 43
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 44 - 43
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 45 - 44
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 45 - 44
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 45 - 44
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 45 - 44
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 46 - 45
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 46 - 45
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 46 - 45
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 46 - 45
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 47 - 46
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 47 - 46
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 47 - 46
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 47 - 46
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 48 - 47
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 48 - 47
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 48 - 47
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 48 - 47
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 49 - 48
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 49 - 48
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 49 - 48
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 49 - 48
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 50 - 49
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 50 - 49
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 50 - 49
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 50 - 49
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 51 - 50
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 51 - 50
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 51 - 50
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 51 - 50
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-Epoch: [0][50/1562]	Memory: 8.167 (12.537)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 411041792.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 0.000 bytes
-Optimizer step took: 0.001
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 52 - 51
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 52 - 51
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 52 - 51
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 52 - 51
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 53 - 52
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 53 - 52
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 53 - 52
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 53 - 52
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 54 - 53
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 54 - 53
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 54 - 53
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 54 - 53
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 55 - 54
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 55 - 54
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 55 - 54
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 55 - 54
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 56 - 55
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 56 - 55
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 56 - 55
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 56 - 55
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 57 - 56
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 57 - 56
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 57 - 56
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 57 - 56
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 58 - 57
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 58 - 57
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 58 - 57
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 58 - 57
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 59 - 58
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 59 - 58
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 59 - 58
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 59 - 58
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 60 - 59
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 60 - 59
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 60 - 59
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 60 - 59
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 61 - 60
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 61 - 60
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 61 - 60
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 61 - 60
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-Epoch: [0][60/1562]	Memory: 8.269 (12.537)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 411041792.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 0.000 bytes
-Optimizer step took: 0.001
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 62 - 61
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 62 - 61
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 62 - 61
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 62 - 61
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 63 - 62
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 63 - 62
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 63 - 62
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 63 - 62
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 64 - 63
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 64 - 63
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 64 - 63
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 64 - 63
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 65 - 64
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 65 - 64
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 65 - 64
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 65 - 64
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 66 - 65
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 66 - 65
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 66 - 65
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 66 - 65
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 67 - 66
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 67 - 66
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 67 - 66
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 67 - 66
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 68 - 67
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 68 - 67
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 68 - 67
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 68 - 67
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 69 - 68
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 69 - 68
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 69 - 68
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 69 - 68
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 70 - 69
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 70 - 69
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 70 - 69
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 70 - 69
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 71 - 70
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 71 - 70
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 71 - 70
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 71 - 70
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-Epoch: [0][70/1562]	Memory: 8.167 (12.537)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 411041792.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 0.000 bytes
-Optimizer step took: 0.001
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 72 - 71
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 72 - 71
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 72 - 71
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 72 - 71
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 73 - 72
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 73 - 72
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 73 - 72
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 73 - 72
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 74 - 73
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 74 - 73
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 74 - 73
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 74 - 73
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 75 - 74
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 75 - 74
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 75 - 74
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 75 - 74
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 76 - 75
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 76 - 75
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 76 - 75
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 76 - 75
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 77 - 76
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 77 - 76
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 77 - 76
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 77 - 76
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 78 - 77
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 78 - 77
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 78 - 77
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 78 - 77
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 79 - 78
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 79 - 78
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 79 - 78
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 79 - 78
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 80 - 79
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 80 - 79
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 80 - 79
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 80 - 79
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 81 - 80
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 81 - 80
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 81 - 80
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 81 - 80
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-Epoch: [0][80/1562]	Memory: 8.167 (12.537)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 411041792.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 0.000 bytes
-Optimizer step took: 0.001
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 82 - 81
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 82 - 81
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 82 - 81
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 82 - 81
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 83 - 82
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 83 - 82
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 83 - 82
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 83 - 82
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 84 - 83
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 84 - 83
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 84 - 83
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 84 - 83
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 85 - 84
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 85 - 84
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 85 - 84
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 85 - 84
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 86 - 85
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 86 - 85
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 86 - 85
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 86 - 85
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 87 - 86
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 87 - 86
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 87 - 86
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 87 - 86
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 88 - 87
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 88 - 87
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 88 - 87
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 88 - 87
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 89 - 88
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 89 - 88
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 89 - 88
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 89 - 88
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 90 - 89
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 90 - 89
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 90 - 89
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 90 - 89
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 91 - 90
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 91 - 90
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 91 - 90
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 91 - 90
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-Epoch: [0][90/1562]	Memory: 8.167 (12.537)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 411041792.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 0.000 bytes
-Optimizer step took: 0.001
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 92 - 91
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 92 - 91
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 92 - 91
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 92 - 91
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 93 - 92
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 93 - 92
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 93 - 92
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 93 - 92
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 94 - 93
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 94 - 93
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 94 - 93
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 94 - 93
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 95 - 94
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 95 - 94
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 95 - 94
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 95 - 94
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 96 - 95
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 96 - 95
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 96 - 95
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 96 - 95
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 97 - 96
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 97 - 96
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 97 - 96
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 97 - 96
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 98 - 97
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 98 - 97
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 98 - 97
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 98 - 97
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 99 - 98
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 99 - 98
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 99 - 98
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 99 - 98
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 100 - 99
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 100 - 99
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 100 - 99
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 100 - 99
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 101 - 100
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 101 - 100
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 101 - 100
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 101 - 100
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-Epoch: 0 Step 100 	Learning rate: 0.010000
-Epoch: [0][100/1562]	Memory: 8.167 (12.537)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 411041792.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 0.000 bytes
-Optimizer step took: 0.001
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 102 - 101
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 102 - 101
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 102 - 101
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 102 - 101
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 103 - 102
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 103 - 102
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 103 - 102
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 103 - 102
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 104 - 103
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 104 - 103
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 104 - 103
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 104 - 103
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 105 - 104
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 105 - 104
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 105 - 104
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 105 - 104
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 106 - 105
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 106 - 105
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 106 - 105
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 106 - 105
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 107 - 106
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 107 - 106
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 107 - 106
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 107 - 106
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 108 - 107
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 108 - 107
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 108 - 107
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 108 - 107
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 109 - 108
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 109 - 108
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 109 - 108
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 109 - 108
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 110 - 109
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 110 - 109
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 110 - 109
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 110 - 109
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 111 - 110
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 111 - 110
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 111 - 110
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 111 - 110
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-Epoch: [0][110/1562]	Memory: 8.167 (12.537)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 411041792.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 0.000 bytes
-Optimizer step took: 0.001
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 112 - 111
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 112 - 111
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 112 - 111
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 112 - 111
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 113 - 112
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 113 - 112
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 113 - 112
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 113 - 112
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 114 - 113
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 114 - 113
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 114 - 113
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 114 - 113
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 115 - 114
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 115 - 114
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 115 - 114
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 115 - 114
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 116 - 115
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 116 - 115
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 116 - 115
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 116 - 115
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 117 - 116
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 117 - 116
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 117 - 116
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 117 - 116
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 118 - 117
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 118 - 117
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 118 - 117
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 118 - 117
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 119 - 118
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 119 - 118
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 119 - 118
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 119 - 118
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 120 - 119
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 120 - 119
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 120 - 119
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 120 - 119
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 121 - 120
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 121 - 120
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 121 - 120
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 121 - 120
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-Epoch: [0][120/1562]	Memory: 8.167 (12.537)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 411041792.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 0.000 bytes
-Optimizer step took: 0.001
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 122 - 121
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 122 - 121
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 122 - 121
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 122 - 121
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 123 - 122
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 123 - 122
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 123 - 122
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 123 - 122
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 124 - 123
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 124 - 123
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 124 - 123
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 124 - 123
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 125 - 124
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 125 - 124
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 125 - 124
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 125 - 124
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 126 - 125
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 126 - 125
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 126 - 125
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 126 - 125
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 127 - 126
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 127 - 126
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 127 - 126
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 127 - 126
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 128 - 127
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 128 - 127
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 128 - 127
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 128 - 127
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 129 - 128
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 129 - 128
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 129 - 128
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 129 - 128
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 130 - 129
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 130 - 129
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 130 - 129
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 130 - 129
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 131 - 130
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 131 - 130
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 131 - 130
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 131 - 130
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-Epoch: [0][130/1562]	Memory: 8.167 (12.537)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 411041792.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 0.000 bytes
-Optimizer step took: 0.001
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 132 - 131
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 132 - 131
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 132 - 131
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 132 - 131
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 133 - 132
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 133 - 132
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 133 - 132
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 133 - 132
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 134 - 133
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 134 - 133
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 134 - 133
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 134 - 133
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 135 - 134
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 135 - 134
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 135 - 134
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 135 - 134
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 136 - 135
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 136 - 135
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 136 - 135
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 136 - 135
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 137 - 136
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 137 - 136
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 137 - 136
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 137 - 136
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 138 - 137
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 138 - 137
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 138 - 137
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 138 - 137
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 139 - 138
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 139 - 138
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 139 - 138
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 139 - 138
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 140 - 139
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 140 - 139
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 140 - 139
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 140 - 139
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 141 - 140
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 141 - 140
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 141 - 140
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 141 - 140
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-Epoch: [0][140/1562]	Memory: 8.167 (12.537)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 411041792.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 0.000 bytes
-Optimizer step took: 0.446
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 142 - 141
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 142 - 141
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 142 - 141
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 142 - 141
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 143 - 142
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 143 - 142
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 143 - 142
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 143 - 142
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 144 - 143
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 144 - 143
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 144 - 143
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 144 - 143
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 145 - 144
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 145 - 144
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 145 - 144
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 145 - 144
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 146 - 145
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 146 - 145
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 146 - 145
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 146 - 145
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 147 - 146
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 147 - 146
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 147 - 146
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 147 - 146
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 148 - 147
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 148 - 147
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 148 - 147
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 148 - 147
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 149 - 148
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 149 - 148
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 149 - 148
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 149 - 148
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 150 - 149
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 150 - 149
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 150 - 149
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 150 - 149
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 151 - 150
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 151 - 150
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 151 - 150
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 151 - 150
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-Epoch: [0][150/1562]	Memory: 8.167 (12.537)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 411041792.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 0.000 bytes
-Optimizer step took: 0.001
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 152 - 151
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 152 - 151
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 152 - 151
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 152 - 151
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 153 - 152
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 153 - 152
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 153 - 152
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 153 - 152
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 154 - 153
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 154 - 153
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 154 - 153
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 154 - 153
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 155 - 154
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 155 - 154
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 155 - 154
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 155 - 154
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 156 - 155
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 156 - 155
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 156 - 155
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 156 - 155
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 157 - 156
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 157 - 156
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 157 - 156
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 157 - 156
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 158 - 157
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 158 - 157
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 158 - 157
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 158 - 157
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 159 - 158
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 159 - 158
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 159 - 158
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 159 - 158
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 160 - 159
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 160 - 159
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 160 - 159
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 160 - 159
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 161 - 160
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 161 - 160
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 161 - 160
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 161 - 160
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-Epoch: [0][160/1562]	Memory: 8.167 (12.537)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 411041792.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 0.000 bytes
-Optimizer step took: 0.001
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 162 - 161
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 162 - 161
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 162 - 161
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 162 - 161
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 163 - 162
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 163 - 162
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 163 - 162
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 163 - 162
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 164 - 163
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 164 - 163
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 164 - 163
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 164 - 163
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 165 - 164
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 165 - 164
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 165 - 164
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 165 - 164
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 166 - 165
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 166 - 165
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 166 - 165
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 166 - 165
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 167 - 166
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 167 - 166
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 167 - 166
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 167 - 166
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 168 - 167
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 168 - 167
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 168 - 167
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 168 - 167
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 169 - 168
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 169 - 168
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 169 - 168
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 169 - 168
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 170 - 169
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 170 - 169
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 170 - 169
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 170 - 169
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 171 - 170
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 171 - 170
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 171 - 170
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 171 - 170
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-Epoch: [0][170/1562]	Memory: 8.269 (12.537)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 411041792.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 0.000 bytes
-Optimizer step took: 0.001
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 172 - 171
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 172 - 171
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 172 - 171
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 172 - 171
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 173 - 172
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 173 - 172
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 173 - 172
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 173 - 172
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 174 - 173
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 174 - 173
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 174 - 173
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 174 - 173
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 175 - 174
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 175 - 174
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 175 - 174
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 175 - 174
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 176 - 175
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 176 - 175
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 176 - 175
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 176 - 175
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 177 - 176
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 177 - 176
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 177 - 176
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 177 - 176
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 178 - 177
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 178 - 177
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 178 - 177
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 178 - 177
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 179 - 178
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 179 - 178
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 179 - 178
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 179 - 178
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 180 - 179
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 180 - 179
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 180 - 179
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 180 - 179
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 181 - 180
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 181 - 180
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 181 - 180
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 181 - 180
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-Epoch: [0][180/1562]	Memory: 8.167 (12.537)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 411041792.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 0.000 bytes
-Optimizer step took: 0.001
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 182 - 181
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 182 - 181
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 182 - 181
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 182 - 181
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 183 - 182
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 183 - 182
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 183 - 182
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 183 - 182
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 184 - 183
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 184 - 183
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 184 - 183
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 184 - 183
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 185 - 184
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 185 - 184
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 185 - 184
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 185 - 184
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 186 - 185
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 186 - 185
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 186 - 185
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 186 - 185
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 187 - 186
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 187 - 186
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 187 - 186
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 187 - 186
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 188 - 187
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 188 - 187
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 188 - 187
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 188 - 187
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 189 - 188
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 189 - 188
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 189 - 188
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 189 - 188
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 190 - 189
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 190 - 189
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 190 - 189
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 190 - 189
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 191 - 190
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 191 - 190
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 191 - 190
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 191 - 190
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-Epoch: [0][190/1562]	Memory: 8.167 (12.537)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 411041792.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 0.000 bytes
-Optimizer step took: 0.001
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 192 - 191
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 192 - 191
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 192 - 191
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 192 - 191
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 193 - 192
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 193 - 192
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 193 - 192
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 193 - 192
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 194 - 193
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 194 - 193
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 194 - 193
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 194 - 193
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 195 - 194
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 195 - 194
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 195 - 194
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 195 - 194
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 196 - 195
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 196 - 195
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 196 - 195
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 196 - 195
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 197 - 196
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 197 - 196
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 197 - 196
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 197 - 196
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 198 - 197
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 198 - 197
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 198 - 197
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 198 - 197
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 199 - 198
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 199 - 198
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 199 - 198
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 199 - 198
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 200 - 199
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 200 - 199
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 200 - 199
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 200 - 199
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 201 - 200
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 201 - 200
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 201 - 200
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 201 - 200
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-Epoch: 0 Step 200 	Learning rate: 0.010000
-Epoch: [0][200/1562]	Memory: 8.168 (12.537)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 411041792.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 0.000 bytes
-Optimizer step took: 0.001
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 202 - 201
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 202 - 201
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 202 - 201
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 202 - 201
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 203 - 202
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 203 - 202
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 203 - 202
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 203 - 202
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 204 - 203
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 204 - 203
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 204 - 203
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 204 - 203
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 205 - 204
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 205 - 204
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 205 - 204
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 205 - 204
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 206 - 205
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 206 - 205
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 206 - 205
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 206 - 205
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 207 - 206
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 207 - 206
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 207 - 206
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 207 - 206
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 208 - 207
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 208 - 207
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 208 - 207
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 208 - 207
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 209 - 208
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 209 - 208
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 209 - 208
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 209 - 208
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 210 - 209
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 210 - 209
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 210 - 209
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 210 - 209
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 211 - 210
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 211 - 210
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 211 - 210
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 211 - 210
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-Epoch: [0][210/1562]	Memory: 8.269 (12.537)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 411041792.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 0.000 bytes
-Optimizer step took: 0.001
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 212 - 211
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 212 - 211
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 212 - 211
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 212 - 211
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 213 - 212
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 213 - 212
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 213 - 212
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 213 - 212
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 214 - 213
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 214 - 213
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 214 - 213
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 214 - 213
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 215 - 214
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 215 - 214
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 215 - 214
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 215 - 214
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 216 - 215
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 216 - 215
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 216 - 215
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 216 - 215
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 217 - 216
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 217 - 216
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 217 - 216
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 217 - 216
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 218 - 217
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 218 - 217
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 218 - 217
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 218 - 217
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 219 - 218
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 219 - 218
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 219 - 218
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 219 - 218
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 220 - 219
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 220 - 219
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 220 - 219
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 220 - 219
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 221 - 220
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 221 - 220
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 221 - 220
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 221 - 220
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-Epoch: [0][220/1562]	Memory: 8.167 (12.537)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 411041792.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 0.000 bytes
-Optimizer step took: 0.001
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 222 - 221
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 222 - 221
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 222 - 221
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 222 - 221
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 223 - 222
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 223 - 222
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 223 - 222
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 223 - 222
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 224 - 223
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 224 - 223
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 224 - 223
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 224 - 223
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 225 - 224
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 225 - 224
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 225 - 224
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 225 - 224
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 226 - 225
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 226 - 225
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 226 - 225
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 226 - 225
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 227 - 226
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 227 - 226
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 227 - 226
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 227 - 226
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 228 - 227
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 228 - 227
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 228 - 227
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 228 - 227
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 229 - 228
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 229 - 228
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 229 - 228
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 229 - 228
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 230 - 229
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 230 - 229
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 230 - 229
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 230 - 229
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 231 - 230
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 231 - 230
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 231 - 230
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 231 - 230
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-Epoch: [0][230/1562]	Memory: 8.167 (12.537)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 411041792.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 0.000 bytes
-Optimizer step took: 0.448
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 232 - 231
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 232 - 231
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 232 - 231
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 232 - 231
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 233 - 232
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 233 - 232
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 233 - 232
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 233 - 232
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 234 - 233
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 234 - 233
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 234 - 233
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 234 - 233
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 235 - 234
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 235 - 234
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 235 - 234
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 235 - 234
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 236 - 235
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 236 - 235
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 236 - 235
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 236 - 235
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 237 - 236
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 237 - 236
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 237 - 236
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 237 - 236
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 238 - 237
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 238 - 237
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 238 - 237
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 238 - 237
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 239 - 238
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 239 - 238
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 239 - 238
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 239 - 238
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 240 - 239
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 240 - 239
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 240 - 239
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 240 - 239
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 241 - 240
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 241 - 240
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 241 - 240
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 241 - 240
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-Epoch: [0][240/1562]	Memory: 8.167 (12.537)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 411041792.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 0.000 bytes
-Optimizer step took: 0.001
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 242 - 241
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 242 - 241
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 242 - 241
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 242 - 241
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 243 - 242
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 243 - 242
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 243 - 242
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 243 - 242
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 244 - 243
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 244 - 243
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 244 - 243
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 244 - 243
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 245 - 244
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 245 - 244
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 245 - 244
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 245 - 244
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 246 - 245
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 246 - 245
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 246 - 245
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 246 - 245
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 247 - 246
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 247 - 246
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 247 - 246
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 247 - 246
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 248 - 247
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 248 - 247
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 248 - 247
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 248 - 247
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 249 - 248
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 249 - 248
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 249 - 248
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 249 - 248
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 250 - 249
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 250 - 249
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 250 - 249
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 250 - 249
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 251 - 250
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 251 - 250
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 251 - 250
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 251 - 250
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-Epoch: [0][250/1562]	Memory: 8.269 (12.537)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 411041792.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 0.000 bytes
-Optimizer step took: 0.001
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 252 - 251
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 252 - 251
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 252 - 251
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 252 - 251
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 253 - 252
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 253 - 252
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 253 - 252
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 253 - 252
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 254 - 253
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 254 - 253
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 254 - 253
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 254 - 253
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 255 - 254
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 255 - 254
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 255 - 254
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 255 - 254
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 256 - 255
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 256 - 255
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 256 - 255
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 256 - 255
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 257 - 256
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 257 - 256
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 257 - 256
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 257 - 256
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 258 - 257
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 258 - 257
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 258 - 257
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 258 - 257
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 259 - 258
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 259 - 258
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 259 - 258
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 259 - 258
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 260 - 259
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 260 - 259
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 260 - 259
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 260 - 259
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 261 - 260
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 261 - 260
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 261 - 260
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 261 - 260
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-Epoch: [0][260/1562]	Memory: 8.167 (12.537)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 411041792.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 0.000 bytes
-Optimizer step took: 0.001
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 262 - 261
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 262 - 261
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 262 - 261
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 262 - 261
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 263 - 262
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 263 - 262
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 263 - 262
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 263 - 262
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 264 - 263
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 264 - 263
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 264 - 263
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 264 - 263
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 265 - 264
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 265 - 264
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 265 - 264
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 265 - 264
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 266 - 265
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 266 - 265
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 266 - 265
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 266 - 265
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 267 - 266
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 267 - 266
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 267 - 266
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 267 - 266
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 268 - 267
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 268 - 267
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 268 - 267
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 268 - 267
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 269 - 268
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 269 - 268
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 269 - 268
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 269 - 268
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 270 - 269
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 270 - 269
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 270 - 269
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 270 - 269
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 271 - 270
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 271 - 270
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 271 - 270
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 271 - 270
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-Epoch: [0][270/1562]	Memory: 8.167 (12.537)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 411041792.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 0.000 bytes
-Optimizer step took: 0.001
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 272 - 271
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 272 - 271
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 272 - 271
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 272 - 271
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 273 - 272
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 273 - 272
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 273 - 272
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 273 - 272
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 274 - 273
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 274 - 273
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 274 - 273
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 274 - 273
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 275 - 274
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 275 - 274
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 275 - 274
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 275 - 274
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 276 - 275
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 276 - 275
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 276 - 275
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 276 - 275
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 277 - 276
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 277 - 276
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 277 - 276
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 277 - 276
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 278 - 277
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 278 - 277
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 278 - 277
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 278 - 277
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 279 - 278
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 279 - 278
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 279 - 278
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 279 - 278
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 280 - 279
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 280 - 279
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 280 - 279
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 280 - 279
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 281 - 280
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 281 - 280
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 281 - 280
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 281 - 280
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-Epoch: [0][280/1562]	Memory: 8.167 (12.537)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 411041792.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 0.000 bytes
-Optimizer step took: 0.001
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 282 - 281
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 282 - 281
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 282 - 281
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 282 - 281
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 283 - 282
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 283 - 282
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 283 - 282
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 283 - 282
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 284 - 283
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 284 - 283
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 284 - 283
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 284 - 283
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 285 - 284
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 285 - 284
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 285 - 284
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 285 - 284
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 286 - 285
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 286 - 285
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 286 - 285
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 286 - 285
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 287 - 286
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 287 - 286
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 287 - 286
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 287 - 286
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 288 - 287
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 288 - 287
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 288 - 287
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 288 - 287
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 289 - 288
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 289 - 288
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 289 - 288
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 289 - 288
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 290 - 289
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 290 - 289
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 290 - 289
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 290 - 289
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 291 - 290
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 291 - 290
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 291 - 290
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 291 - 290
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-Epoch: [0][290/1562]	Memory: 8.167 (12.537)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 411041792.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 0.000 bytes
-Optimizer step took: 0.001
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 292 - 291
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 292 - 291
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 292 - 291
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 292 - 291
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 293 - 292
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 293 - 292
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 293 - 292
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 293 - 292
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 294 - 293
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 294 - 293
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 294 - 293
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 294 - 293
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 295 - 294
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 295 - 294
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 295 - 294
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 295 - 294
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 296 - 295
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 296 - 295
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 296 - 295
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 296 - 295
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 297 - 296
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 297 - 296
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 297 - 296
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 297 - 296
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 298 - 297
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 298 - 297
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 298 - 297
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 298 - 297
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 299 - 298
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 299 - 298
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 299 - 298
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 299 - 298
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 300 - 299
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 300 - 299
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 300 - 299
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 300 - 299
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 301 - 300
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 301 - 300
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 301 - 300
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 301 - 300
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-Epoch: 0 Step 300 	Learning rate: 0.010000
-Epoch: [0][300/1562]	Memory: 8.167 (12.537)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 411041792.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 0.000 bytes
-Optimizer step took: 0.001
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 302 - 301
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 302 - 301
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 302 - 301
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 302 - 301
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 303 - 302
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 303 - 302
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 303 - 302
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 303 - 302
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 304 - 303
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 304 - 303
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 304 - 303
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 304 - 303
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 305 - 304
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 305 - 304
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 305 - 304
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 305 - 304
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 306 - 305
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 306 - 305
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 306 - 305
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 306 - 305
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 307 - 306
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 307 - 306
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 307 - 306
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 307 - 306
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 308 - 307
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 308 - 307
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 308 - 307
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 308 - 307
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 309 - 308
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 309 - 308
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 309 - 308
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 309 - 308
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 310 - 309
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 310 - 309
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 310 - 309
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 310 - 309
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 311 - 310
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 311 - 310
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 311 - 310
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 311 - 310
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-Epoch: [0][310/1562]	Memory: 8.167 (12.537)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 411041792.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 0.000 bytes
-Optimizer step took: 0.001
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 312 - 311
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 312 - 311
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 312 - 311
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 312 - 311
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 313 - 312
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 313 - 312
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 313 - 312
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 313 - 312
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 314 - 313
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 314 - 313
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 314 - 313
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 314 - 313
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 315 - 314
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 315 - 314
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 315 - 314
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 315 - 314
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 316 - 315
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 316 - 315
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 316 - 315
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 316 - 315
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 317 - 316
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 317 - 316
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 317 - 316
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 317 - 316
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 318 - 317
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 318 - 317
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 318 - 317
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 318 - 317
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 319 - 318
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 319 - 318
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 319 - 318
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 319 - 318
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 320 - 319
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 320 - 319
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 320 - 319
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 320 - 319
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 321 - 320
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 321 - 320
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 321 - 320
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 321 - 320
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-Epoch: [0][320/1562]	Memory: 8.269 (12.537)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 411041792.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 0.000 bytes
-Optimizer step took: 0.001
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 322 - 321
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 322 - 321
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 322 - 321
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 322 - 321
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 323 - 322
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 323 - 322
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 323 - 322
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 323 - 322
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 324 - 323
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 324 - 323
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 324 - 323
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 324 - 323
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 325 - 324
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 325 - 324
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 325 - 324
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 325 - 324
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 326 - 325
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 326 - 325
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 326 - 325
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 326 - 325
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 327 - 326
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 327 - 326
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 327 - 326
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 327 - 326
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 328 - 327
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 328 - 327
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 328 - 327
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 328 - 327
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 329 - 328
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 329 - 328
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 329 - 328
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 329 - 328
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 330 - 329
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 330 - 329
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 330 - 329
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 330 - 329
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 331 - 330
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 331 - 330
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 331 - 330
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 331 - 330
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-Epoch: [0][330/1562]	Memory: 8.269 (12.537)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 411041792.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 0.000 bytes
-Optimizer step took: 0.001
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 332 - 331
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 332 - 331
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 332 - 331
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 332 - 331
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 333 - 332
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 333 - 332
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 333 - 332
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 333 - 332
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 334 - 333
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 334 - 333
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 334 - 333
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 334 - 333
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 335 - 334
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 335 - 334
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 335 - 334
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 335 - 334
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 336 - 335
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 336 - 335
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 336 - 335
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 336 - 335
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 337 - 336
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 337 - 336
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 337 - 336
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 337 - 336
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 338 - 337
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 338 - 337
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 338 - 337
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 338 - 337
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 339 - 338
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 339 - 338
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 339 - 338
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 339 - 338
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 340 - 339
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 340 - 339
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 340 - 339
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 340 - 339
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 341 - 340
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 341 - 340
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 341 - 340
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 341 - 340
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-Epoch: [0][340/1562]	Memory: 8.167 (12.537)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 411041792.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 0.000 bytes
-Optimizer step took: 0.006
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 342 - 341
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 342 - 341
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 342 - 341
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 342 - 341
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 343 - 342
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 343 - 342
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 343 - 342
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 343 - 342
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 344 - 343
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 344 - 343
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 344 - 343
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 344 - 343
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 345 - 344
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 345 - 344
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 345 - 344
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 345 - 344
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 346 - 345
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 346 - 345
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 346 - 345
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 346 - 345
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 347 - 346
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 347 - 346
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 347 - 346
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 347 - 346
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 348 - 347
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 348 - 347
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 348 - 347
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 348 - 347
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 349 - 348
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 349 - 348
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 349 - 348
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 349 - 348
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 350 - 349
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 350 - 349
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 350 - 349
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 350 - 349
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 351 - 350
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 351 - 350
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 351 - 350
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 351 - 350
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-Epoch: [0][350/1562]	Memory: 8.167 (12.537)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 411041792.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 0.000 bytes
-Optimizer step took: 0.001
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 352 - 351
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 352 - 351
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 352 - 351
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 352 - 351
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 353 - 352
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 353 - 352
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 353 - 352
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 353 - 352
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 354 - 353
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 354 - 353
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 354 - 353
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 354 - 353
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 355 - 354
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 355 - 354
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 355 - 354
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 355 - 354
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 356 - 355
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 356 - 355
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 356 - 355
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 356 - 355
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 357 - 356
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 357 - 356
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 357 - 356
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 357 - 356
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 358 - 357
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 358 - 357
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 358 - 357
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 358 - 357
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 359 - 358
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 359 - 358
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 359 - 358
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 359 - 358
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 360 - 359
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 360 - 359
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 360 - 359
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 360 - 359
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 361 - 360
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 361 - 360
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 361 - 360
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 361 - 360
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-Epoch: [0][360/1562]	Memory: 8.269 (12.537)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 411041792.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 0.000 bytes
-Optimizer step took: 0.001
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 362 - 361
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 362 - 361
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 362 - 361
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 362 - 361
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 363 - 362
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 363 - 362
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 363 - 362
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 363 - 362
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 364 - 363
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 364 - 363
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 364 - 363
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 364 - 363
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 365 - 364
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 365 - 364
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 365 - 364
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 365 - 364
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 366 - 365
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 366 - 365
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 366 - 365
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 366 - 365
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 367 - 366
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 367 - 366
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 367 - 366
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 367 - 366
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 368 - 367
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 368 - 367
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 368 - 367
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 368 - 367
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 369 - 368
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 369 - 368
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 369 - 368
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 369 - 368
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 370 - 369
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 370 - 369
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 370 - 369
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 370 - 369
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 371 - 370
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 371 - 370
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 371 - 370
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 371 - 370
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-Epoch: [0][370/1562]	Memory: 8.167 (12.537)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 411041792.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 0.000 bytes
-Optimizer step took: 0.001
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 372 - 371
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 372 - 371
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 372 - 371
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 372 - 371
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 373 - 372
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 373 - 372
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 373 - 372
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 373 - 372
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 374 - 373
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 374 - 373
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 374 - 373
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 374 - 373
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 375 - 374
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 375 - 374
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 375 - 374
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 375 - 374
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 376 - 375
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 376 - 375
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 376 - 375
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 376 - 375
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 377 - 376
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 377 - 376
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 377 - 376
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 377 - 376
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 378 - 377
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 378 - 377
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 378 - 377
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 378 - 377
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 379 - 378
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 379 - 378
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 379 - 378
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 379 - 378
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 380 - 379
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 380 - 379
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 380 - 379
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 380 - 379
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 381 - 380
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 381 - 380
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 381 - 380
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 381 - 380
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-Epoch: [0][380/1562]	Memory: 8.269 (12.537)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 411041792.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 0.000 bytes
-Optimizer step took: 0.001
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 382 - 381
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 382 - 381
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 382 - 381
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 382 - 381
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 383 - 382
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 383 - 382
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 383 - 382
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 383 - 382
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 384 - 383
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 384 - 383
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 384 - 383
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 384 - 383
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 385 - 384
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 385 - 384
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 385 - 384
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 385 - 384
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 386 - 385
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 386 - 385
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 386 - 385
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 386 - 385
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 387 - 386
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 387 - 386
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 387 - 386
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 387 - 386
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 388 - 387
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 388 - 387
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 388 - 387
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 388 - 387
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 389 - 388
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 389 - 388
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 389 - 388
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 389 - 388
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 390 - 389
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 390 - 389
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 390 - 389
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 390 - 389
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-enter run_forward
-enter _run_forward
-Start upstream tail layer
-enter send_block f_mini-b_mini: 391 - 390
-block0: inp: torch.Size([64, 128, 59, 59]) out: torch.Size([64, 128, 57, 57])
-enter send_block f_mini-b_mini: 391 - 390
-block1: inp: torch.Size([64, 128, 59, 57]) out: torch.Size([64, 128, 57, 55])
-enter send_block f_mini-b_mini: 391 - 390
-block2: inp: torch.Size([64, 128, 57, 59]) out: torch.Size([64, 128, 55, 57])
-enter send_block f_mini-b_mini: 391 - 390
-block3: inp: torch.Size([64, 128, 57, 57]) out: torch.Size([64, 128, 55, 55])
-skip send: out0
-try send: target
-sent: target
-Epoch: [0][390/1562]	Memory: 8.270 (12.537)
diff --git a/archive_logs/2020-04-27T00:41:40_stage0/vgg16_2mp_exp.yml b/archive_logs/2020-04-27T00:41:40_stage0/vgg16_2mp_exp.yml
deleted file mode 100644
index 4d9ce0b..0000000
--- a/archive_logs/2020-04-27T00:41:40_stage0/vgg16_2mp_exp.yml
+++ /dev/null
@@ -1,17 +0,0 @@
-'log_directory': '/home/ubuntu/pipedream/output_logs'
-'module': 'models.vgg16.gpus=2'
-'data_dir': '/home/ubuntu/data/imagenet_tiny/'
-'config_file': 'models/vgg16/gpus=2/mp_conf.json'
-'container': 'nvcr.io/nvidia/pytorch:19.05-py3'
-'machines': ['172.31.15.10:0', '172.31.12.252:1']
-'batch_size': 64
-'learning_rate': 0.01
-'weight_decay': 0.0005
-'epochs': 60
-'print_frequency': 10
-'verbose_frequency': 10
-'compress_activations': False
-'compress_in_gpu': False
-'learning_rate_policy': 'polynomial'
-'model_type': 'image_classification'
-'distributed_backend': 'gloo'
diff --git a/archive_logs/2020-04-27T00:41:40_stage1/output.log.1 b/archive_logs/2020-04-27T00:41:40_stage1/output.log.1
deleted file mode 100644
index b1ce2c3..0000000
--- a/archive_logs/2020-04-27T00:41:40_stage1/output.log.1
+++ /dev/null
@@ -1,5794 +0,0 @@
-initialize upstream tail module
-initialize downstream head module
-Finished initializing process group; backend: gloo, rank: 1, world_size: 2
-Send ranks:  {}
-Receive ranks:  {'out0': [0], 'target': [0]}
-Letting in 0 warm-up minibatches
-Running training for 1562 minibatches
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 0 - 0
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 0 - 0
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 0 - 0
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 0 - 0
-recv: tensor: torch.Size([64, 128, 55, 55])
-Epoch: 0 Step 0 	Learning rate: 0.010000
-Epoch: [0][0/1562]	Time: 7.033 (7.033)	Epoch time [hr]: 0.002 (3.052)	Memory: 4.731 (4.958)	Loss: 6.9167 (6.9167)	Prec@1: 0.000 (0.000)	Prec@5: 0.000 (0.000)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 0.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 411041792.000 bytes
-Optimizer step took: 0.399
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 1 - 1
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 1 - 1
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 1 - 1
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 1 - 1
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 2 - 2
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 2 - 2
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 2 - 2
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 2 - 2
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 3 - 3
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 3 - 3
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 3 - 3
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 3 - 3
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 4 - 4
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 4 - 4
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 4 - 4
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 4 - 4
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 5 - 5
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 5 - 5
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 5 - 5
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 5 - 5
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 6 - 6
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 6 - 6
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 6 - 6
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 6 - 6
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 7 - 7
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 7 - 7
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 7 - 7
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 7 - 7
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 8 - 8
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 8 - 8
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 8 - 8
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 8 - 8
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 9 - 9
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 9 - 9
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 9 - 9
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 9 - 9
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 10 - 10
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 10 - 10
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 10 - 10
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 10 - 10
-recv: tensor: torch.Size([64, 128, 55, 55])
-Epoch: [0][10/1562]	Time: 1.460 (2.717)	Epoch time [hr]: 0.008 (1.179)	Memory: 5.836 (7.824)	Loss: 6.8181 (6.8890)	Prec@1: 0.000 (0.142)	Prec@5: 7.812 (1.136)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 0.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 411041792.000 bytes
-Optimizer step took: 0.683
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 11 - 11
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 11 - 11
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 11 - 11
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 11 - 11
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 12 - 12
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 12 - 12
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 12 - 12
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 12 - 12
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 13 - 13
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 13 - 13
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 13 - 13
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 13 - 13
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 14 - 14
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 14 - 14
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 14 - 14
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 14 - 14
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 15 - 15
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 15 - 15
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 15 - 15
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 15 - 15
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 16 - 16
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 16 - 16
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 16 - 16
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 16 - 16
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 17 - 17
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 17 - 17
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 17 - 17
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 17 - 17
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 18 - 18
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 18 - 18
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 18 - 18
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 18 - 18
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 19 - 19
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 19 - 19
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 19 - 19
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 19 - 19
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 20 - 20
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 20 - 20
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 20 - 20
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 20 - 20
-recv: tensor: torch.Size([64, 128, 55, 55])
-Epoch: [0][20/1562]	Time: 2.115 (2.334)	Epoch time [hr]: 0.014 (1.012)	Memory: 5.836 (7.824)	Loss: 6.1955 (6.6862)	Prec@1: 1.562 (0.298)	Prec@5: 1.562 (1.488)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 0.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 411041792.000 bytes
-Optimizer step took: 0.667
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 21 - 21
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 21 - 21
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 21 - 21
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 21 - 21
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 22 - 22
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 22 - 22
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 22 - 22
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 22 - 22
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 23 - 23
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 23 - 23
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 23 - 23
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 23 - 23
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 24 - 24
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 24 - 24
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 24 - 24
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 24 - 24
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 25 - 25
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 25 - 25
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 25 - 25
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 25 - 25
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 26 - 26
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 26 - 26
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 26 - 26
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 26 - 26
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 27 - 27
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 27 - 27
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 27 - 27
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 27 - 27
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 28 - 28
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 28 - 28
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 28 - 28
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 28 - 28
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 29 - 29
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 29 - 29
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 29 - 29
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 29 - 29
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 30 - 30
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 30 - 30
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 30 - 30
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 30 - 30
-recv: tensor: torch.Size([64, 128, 55, 55])
-Epoch: [0][30/1562]	Time: 2.102 (2.215)	Epoch time [hr]: 0.019 (0.961)	Memory: 5.836 (7.824)	Loss: 6.0301 (6.5335)	Prec@1: 0.000 (0.252)	Prec@5: 3.125 (1.663)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 0.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 411041792.000 bytes
-Optimizer step took: 0.663
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 31 - 31
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 31 - 31
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 31 - 31
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 31 - 31
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 32 - 32
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 32 - 32
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 32 - 32
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 32 - 32
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 33 - 33
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 33 - 33
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 33 - 33
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 33 - 33
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 34 - 34
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 34 - 34
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 34 - 34
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 34 - 34
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 35 - 35
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 35 - 35
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 35 - 35
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 35 - 35
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 36 - 36
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 36 - 36
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 36 - 36
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 36 - 36
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 37 - 37
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 37 - 37
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 37 - 37
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 37 - 37
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 38 - 38
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 38 - 38
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 38 - 38
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 38 - 38
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 39 - 39
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 39 - 39
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 39 - 39
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 39 - 39
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 40 - 40
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 40 - 40
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 40 - 40
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 40 - 40
-recv: tensor: torch.Size([64, 128, 55, 55])
-Epoch: [0][40/1562]	Time: 2.115 (2.160)	Epoch time [hr]: 0.025 (0.937)	Memory: 5.836 (7.824)	Loss: 5.6780 (6.4021)	Prec@1: 0.000 (0.267)	Prec@5: 1.562 (1.715)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 0.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 411041792.000 bytes
-Optimizer step took: 0.668
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 41 - 41
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 41 - 41
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 41 - 41
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 41 - 41
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 42 - 42
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 42 - 42
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 42 - 42
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 42 - 42
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 43 - 43
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 43 - 43
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 43 - 43
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 43 - 43
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 44 - 44
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 44 - 44
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 44 - 44
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 44 - 44
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 45 - 45
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 45 - 45
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 45 - 45
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 45 - 45
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 46 - 46
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 46 - 46
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 46 - 46
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 46 - 46
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 47 - 47
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 47 - 47
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 47 - 47
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 47 - 47
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 48 - 48
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 48 - 48
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 48 - 48
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 48 - 48
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 49 - 49
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 49 - 49
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 49 - 49
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 49 - 49
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 50 - 50
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 50 - 50
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 50 - 50
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 50 - 50
-recv: tensor: torch.Size([64, 128, 55, 55])
-Epoch: [0][50/1562]	Time: 2.023 (2.132)	Epoch time [hr]: 0.030 (0.925)	Memory: 5.836 (7.824)	Loss: 5.6235 (6.2721)	Prec@1: 0.000 (0.306)	Prec@5: 3.125 (1.991)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 0.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 411041792.000 bytes
-Optimizer step took: 0.657
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 51 - 51
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 51 - 51
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 51 - 51
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 51 - 51
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 52 - 52
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 52 - 52
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 52 - 52
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 52 - 52
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 53 - 53
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 53 - 53
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 53 - 53
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 53 - 53
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 54 - 54
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 54 - 54
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 54 - 54
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 54 - 54
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 55 - 55
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 55 - 55
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 55 - 55
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 55 - 55
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 56 - 56
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 56 - 56
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 56 - 56
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 56 - 56
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 57 - 57
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 57 - 57
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 57 - 57
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 57 - 57
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 58 - 58
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 58 - 58
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 58 - 58
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 58 - 58
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 59 - 59
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 59 - 59
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 59 - 59
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 59 - 59
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 60 - 60
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 60 - 60
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 60 - 60
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 60 - 60
-recv: tensor: torch.Size([64, 128, 55, 55])
-Epoch: [0][60/1562]	Time: 2.102 (2.122)	Epoch time [hr]: 0.036 (0.921)	Memory: 5.836 (7.824)	Loss: 5.6919 (6.1768)	Prec@1: 0.000 (0.307)	Prec@5: 0.000 (2.049)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 0.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 411041792.000 bytes
-Optimizer step took: 0.659
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 61 - 61
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 61 - 61
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 61 - 61
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 61 - 61
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 62 - 62
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 62 - 62
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 62 - 62
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 62 - 62
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 63 - 63
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 63 - 63
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 63 - 63
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 63 - 63
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 64 - 64
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 64 - 64
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 64 - 64
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 64 - 64
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 65 - 65
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 65 - 65
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 65 - 65
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 65 - 65
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 66 - 66
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 66 - 66
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 66 - 66
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 66 - 66
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 67 - 67
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 67 - 67
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 67 - 67
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 67 - 67
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 68 - 68
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 68 - 68
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 68 - 68
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 68 - 68
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 69 - 69
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 69 - 69
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 69 - 69
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 69 - 69
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 70 - 70
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 70 - 70
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 70 - 70
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 70 - 70
-recv: tensor: torch.Size([64, 128, 55, 55])
-Epoch: [0][70/1562]	Time: 1.593 (2.085)	Epoch time [hr]: 0.041 (0.905)	Memory: 5.836 (7.824)	Loss: 5.5118 (6.0960)	Prec@1: 1.562 (0.374)	Prec@5: 3.125 (2.113)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 0.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 411041792.000 bytes
-Optimizer step took: 0.714
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 71 - 71
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 71 - 71
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 71 - 71
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 71 - 71
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 72 - 72
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 72 - 72
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 72 - 72
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 72 - 72
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 73 - 73
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 73 - 73
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 73 - 73
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 73 - 73
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 74 - 74
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 74 - 74
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 74 - 74
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 74 - 74
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 75 - 75
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 75 - 75
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 75 - 75
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 75 - 75
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 76 - 76
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 76 - 76
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 76 - 76
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 76 - 76
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 77 - 77
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 77 - 77
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 77 - 77
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 77 - 77
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 78 - 78
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 78 - 78
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 78 - 78
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 78 - 78
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 79 - 79
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 79 - 79
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 79 - 79
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 79 - 79
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 80 - 80
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 80 - 80
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 80 - 80
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 80 - 80
-recv: tensor: torch.Size([64, 128, 55, 55])
-Epoch: [0][80/1562]	Time: 2.103 (2.053)	Epoch time [hr]: 0.046 (0.891)	Memory: 5.836 (7.824)	Loss: 5.5666 (6.0319)	Prec@1: 0.000 (0.444)	Prec@5: 0.000 (2.180)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 0.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 411041792.000 bytes
-Optimizer step took: 0.655
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 81 - 81
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 81 - 81
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 81 - 81
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 81 - 81
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 82 - 82
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 82 - 82
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 82 - 82
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 82 - 82
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 83 - 83
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 83 - 83
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 83 - 83
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 83 - 83
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 84 - 84
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 84 - 84
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 84 - 84
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 84 - 84
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 85 - 85
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 85 - 85
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 85 - 85
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 85 - 85
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 86 - 86
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 86 - 86
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 86 - 86
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 86 - 86
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 87 - 87
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 87 - 87
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 87 - 87
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 87 - 87
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 88 - 88
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 88 - 88
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 88 - 88
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 88 - 88
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 89 - 89
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 89 - 89
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 89 - 89
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 89 - 89
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 90 - 90
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 90 - 90
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 90 - 90
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 90 - 90
-recv: tensor: torch.Size([64, 128, 55, 55])
-Epoch: [0][90/1562]	Time: 1.597 (2.039)	Epoch time [hr]: 0.052 (0.885)	Memory: 5.836 (7.824)	Loss: 5.5375 (5.9787)	Prec@1: 0.000 (0.464)	Prec@5: 1.562 (2.181)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 0.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 411041792.000 bytes
-Optimizer step took: 0.664
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 91 - 91
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 91 - 91
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 91 - 91
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 91 - 91
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 92 - 92
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 92 - 92
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 92 - 92
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 92 - 92
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 93 - 93
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 93 - 93
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 93 - 93
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 93 - 93
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 94 - 94
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 94 - 94
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 94 - 94
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 94 - 94
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 95 - 95
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 95 - 95
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 95 - 95
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 95 - 95
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 96 - 96
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 96 - 96
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 96 - 96
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 96 - 96
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 97 - 97
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 97 - 97
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 97 - 97
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 97 - 97
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 98 - 98
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 98 - 98
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 98 - 98
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 98 - 98
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 99 - 99
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 99 - 99
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 99 - 99
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 99 - 99
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 100 - 100
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 100 - 100
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 100 - 100
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 100 - 100
-recv: tensor: torch.Size([64, 128, 55, 55])
-Epoch: 0 Step 100 	Learning rate: 0.010000
-Epoch: [0][100/1562]	Time: 1.940 (2.032)	Epoch time [hr]: 0.057 (0.882)	Memory: 5.836 (7.824)	Loss: 5.6143 (5.9360)	Prec@1: 0.000 (0.449)	Prec@5: 0.000 (2.135)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 0.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 411041792.000 bytes
-Optimizer step took: 0.668
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 101 - 101
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 101 - 101
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 101 - 101
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 101 - 101
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 102 - 102
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 102 - 102
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 102 - 102
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 102 - 102
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 103 - 103
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 103 - 103
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 103 - 103
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 103 - 103
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 104 - 104
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 104 - 104
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 104 - 104
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 104 - 104
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 105 - 105
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 105 - 105
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 105 - 105
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 105 - 105
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 106 - 106
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 106 - 106
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 106 - 106
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 106 - 106
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 107 - 107
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 107 - 107
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 107 - 107
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 107 - 107
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 108 - 108
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 108 - 108
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 108 - 108
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 108 - 108
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 109 - 109
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 109 - 109
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 109 - 109
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 109 - 109
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 110 - 110
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 110 - 110
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 110 - 110
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 110 - 110
-recv: tensor: torch.Size([64, 128, 55, 55])
-Epoch: [0][110/1562]	Time: 1.575 (2.015)	Epoch time [hr]: 0.062 (0.874)	Memory: 5.836 (7.824)	Loss: 5.6320 (5.9018)	Prec@1: 0.000 (0.436)	Prec@5: 1.562 (2.168)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 0.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 411041792.000 bytes
-Optimizer step took: 0.662
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 111 - 111
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 111 - 111
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 111 - 111
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 111 - 111
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 112 - 112
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 112 - 112
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 112 - 112
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 112 - 112
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 113 - 113
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 113 - 113
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 113 - 113
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 113 - 113
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 114 - 114
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 114 - 114
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 114 - 114
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 114 - 114
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 115 - 115
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 115 - 115
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 115 - 115
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 115 - 115
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 116 - 116
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 116 - 116
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 116 - 116
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 116 - 116
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 117 - 117
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 117 - 117
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 117 - 117
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 117 - 117
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 118 - 118
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 118 - 118
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 118 - 118
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 118 - 118
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 119 - 119
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 119 - 119
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 119 - 119
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 119 - 119
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 120 - 120
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 120 - 120
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 120 - 120
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 120 - 120
-recv: tensor: torch.Size([64, 128, 55, 55])
-Epoch: [0][120/1562]	Time: 1.593 (2.007)	Epoch time [hr]: 0.067 (0.871)	Memory: 5.836 (7.824)	Loss: 5.3852 (5.8693)	Prec@1: 0.000 (0.478)	Prec@5: 3.125 (2.234)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 0.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 411041792.000 bytes
-Optimizer step took: 0.716
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 121 - 121
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 121 - 121
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 121 - 121
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 121 - 121
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 122 - 122
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 122 - 122
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 122 - 122
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 122 - 122
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 123 - 123
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 123 - 123
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 123 - 123
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 123 - 123
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 124 - 124
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 124 - 124
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 124 - 124
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 124 - 124
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 125 - 125
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 125 - 125
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 125 - 125
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 125 - 125
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 126 - 126
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 126 - 126
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 126 - 126
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 126 - 126
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 127 - 127
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 127 - 127
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 127 - 127
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 127 - 127
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 128 - 128
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 128 - 128
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 128 - 128
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 128 - 128
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 129 - 129
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 129 - 129
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 129 - 129
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 129 - 129
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 130 - 130
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 130 - 130
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 130 - 130
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 130 - 130
-recv: tensor: torch.Size([64, 128, 55, 55])
-Epoch: [0][130/1562]	Time: 1.616 (1.998)	Epoch time [hr]: 0.073 (0.867)	Memory: 5.836 (7.824)	Loss: 5.4514 (5.8397)	Prec@1: 3.125 (0.477)	Prec@5: 7.812 (2.302)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 0.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 411041792.000 bytes
-Optimizer step took: 0.714
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 131 - 131
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 131 - 131
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 131 - 131
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 131 - 131
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 132 - 132
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 132 - 132
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 132 - 132
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 132 - 132
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 133 - 133
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 133 - 133
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 133 - 133
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 133 - 133
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 134 - 134
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 134 - 134
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 134 - 134
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 134 - 134
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 135 - 135
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 135 - 135
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 135 - 135
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 135 - 135
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 136 - 136
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 136 - 136
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 136 - 136
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 136 - 136
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 137 - 137
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 137 - 137
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 137 - 137
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 137 - 137
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 138 - 138
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 138 - 138
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 138 - 138
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 138 - 138
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 139 - 139
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 139 - 139
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 139 - 139
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 139 - 139
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 140 - 140
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 140 - 140
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 140 - 140
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 140 - 140
-recv: tensor: torch.Size([64, 128, 55, 55])
-Epoch: [0][140/1562]	Time: 2.087 (1.988)	Epoch time [hr]: 0.078 (0.862)	Memory: 5.836 (7.824)	Loss: 5.4992 (5.8157)	Prec@1: 1.562 (0.477)	Prec@5: 3.125 (2.371)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 0.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 411041792.000 bytes
-Optimizer step took: 0.657
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 141 - 141
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 141 - 141
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 141 - 141
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 141 - 141
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 142 - 142
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 142 - 142
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 142 - 142
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 142 - 142
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 143 - 143
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 143 - 143
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 143 - 143
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 143 - 143
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 144 - 144
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 144 - 144
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 144 - 144
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 144 - 144
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 145 - 145
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 145 - 145
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 145 - 145
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 145 - 145
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 146 - 146
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 146 - 146
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 146 - 146
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 146 - 146
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 147 - 147
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 147 - 147
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 147 - 147
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 147 - 147
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 148 - 148
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 148 - 148
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 148 - 148
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 148 - 148
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 149 - 149
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 149 - 149
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 149 - 149
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 149 - 149
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 150 - 150
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 150 - 150
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 150 - 150
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 150 - 150
-recv: tensor: torch.Size([64, 128, 55, 55])
-Epoch: [0][150/1562]	Time: 1.956 (1.983)	Epoch time [hr]: 0.083 (0.860)	Memory: 5.836 (7.824)	Loss: 5.4883 (5.7936)	Prec@1: 0.000 (0.507)	Prec@5: 0.000 (2.432)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 0.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 411041792.000 bytes
-Optimizer step took: 0.657
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 151 - 151
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 151 - 151
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 151 - 151
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 151 - 151
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 152 - 152
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 152 - 152
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 152 - 152
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 152 - 152
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 153 - 153
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 153 - 153
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 153 - 153
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 153 - 153
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 154 - 154
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 154 - 154
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 154 - 154
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 154 - 154
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 155 - 155
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 155 - 155
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 155 - 155
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 155 - 155
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 156 - 156
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 156 - 156
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 156 - 156
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 156 - 156
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 157 - 157
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 157 - 157
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 157 - 157
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 157 - 157
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 158 - 158
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 158 - 158
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 158 - 158
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 158 - 158
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 159 - 159
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 159 - 159
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 159 - 159
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 159 - 159
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 160 - 160
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 160 - 160
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 160 - 160
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 160 - 160
-recv: tensor: torch.Size([64, 128, 55, 55])
-Epoch: [0][160/1562]	Time: 1.610 (1.970)	Epoch time [hr]: 0.088 (0.855)	Memory: 5.836 (7.824)	Loss: 5.4947 (5.7735)	Prec@1: 0.000 (0.476)	Prec@5: 0.000 (2.426)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 0.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 411041792.000 bytes
-Optimizer step took: 0.664
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 161 - 161
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 161 - 161
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 161 - 161
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 161 - 161
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 162 - 162
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 162 - 162
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 162 - 162
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 162 - 162
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 163 - 163
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 163 - 163
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 163 - 163
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 163 - 163
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 164 - 164
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 164 - 164
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 164 - 164
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 164 - 164
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 165 - 165
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 165 - 165
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 165 - 165
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 165 - 165
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 166 - 166
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 166 - 166
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 166 - 166
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 166 - 166
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 167 - 167
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 167 - 167
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 167 - 167
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 167 - 167
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 168 - 168
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 168 - 168
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 168 - 168
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 168 - 168
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 169 - 169
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 169 - 169
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 169 - 169
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 169 - 169
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 170 - 170
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 170 - 170
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 170 - 170
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 170 - 170
-recv: tensor: torch.Size([64, 128, 55, 55])
-Epoch: [0][170/1562]	Time: 2.116 (1.975)	Epoch time [hr]: 0.094 (0.857)	Memory: 5.836 (7.824)	Loss: 5.4746 (5.7563)	Prec@1: 0.000 (0.503)	Prec@5: 4.688 (2.485)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 0.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 411041792.000 bytes
-Optimizer step took: 0.659
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 171 - 171
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 171 - 171
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 171 - 171
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 171 - 171
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 172 - 172
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 172 - 172
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 172 - 172
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 172 - 172
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 173 - 173
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 173 - 173
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 173 - 173
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 173 - 173
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 174 - 174
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 174 - 174
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 174 - 174
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 174 - 174
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 175 - 175
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 175 - 175
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 175 - 175
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 175 - 175
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 176 - 176
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 176 - 176
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 176 - 176
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 176 - 176
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 177 - 177
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 177 - 177
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 177 - 177
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 177 - 177
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 178 - 178
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 178 - 178
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 178 - 178
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 178 - 178
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 179 - 179
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 179 - 179
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 179 - 179
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 179 - 179
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 180 - 180
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 180 - 180
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 180 - 180
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 180 - 180
-recv: tensor: torch.Size([64, 128, 55, 55])
-Epoch: [0][180/1562]	Time: 2.016 (1.981)	Epoch time [hr]: 0.100 (0.860)	Memory: 5.836 (7.824)	Loss: 5.5140 (5.7405)	Prec@1: 1.562 (0.527)	Prec@5: 1.562 (2.529)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 0.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 411041792.000 bytes
-Optimizer step took: 0.660
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 181 - 181
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 181 - 181
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 181 - 181
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 181 - 181
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 182 - 182
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 182 - 182
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 182 - 182
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 182 - 182
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 183 - 183
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 183 - 183
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 183 - 183
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 183 - 183
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 184 - 184
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 184 - 184
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 184 - 184
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 184 - 184
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 185 - 185
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 185 - 185
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 185 - 185
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 185 - 185
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 186 - 186
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 186 - 186
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 186 - 186
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 186 - 186
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 187 - 187
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 187 - 187
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 187 - 187
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 187 - 187
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 188 - 188
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 188 - 188
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 188 - 188
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 188 - 188
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 189 - 189
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 189 - 189
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 189 - 189
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 189 - 189
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 190 - 190
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 190 - 190
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 190 - 190
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 190 - 190
-recv: tensor: torch.Size([64, 128, 55, 55])
-Epoch: [0][190/1562]	Time: 1.953 (1.984)	Epoch time [hr]: 0.105 (0.861)	Memory: 5.836 (7.824)	Loss: 5.4310 (5.7261)	Prec@1: 0.000 (0.524)	Prec@5: 1.562 (2.511)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 0.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 411041792.000 bytes
-Optimizer step took: 0.655
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 191 - 191
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 191 - 191
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 191 - 191
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 191 - 191
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 192 - 192
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 192 - 192
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 192 - 192
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 192 - 192
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 193 - 193
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 193 - 193
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 193 - 193
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 193 - 193
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 194 - 194
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 194 - 194
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 194 - 194
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 194 - 194
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 195 - 195
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 195 - 195
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 195 - 195
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 195 - 195
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 196 - 196
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 196 - 196
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 196 - 196
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 196 - 196
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 197 - 197
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 197 - 197
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 197 - 197
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 197 - 197
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 198 - 198
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 198 - 198
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 198 - 198
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 198 - 198
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 199 - 199
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 199 - 199
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 199 - 199
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 199 - 199
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 200 - 200
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 200 - 200
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 200 - 200
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 200 - 200
-recv: tensor: torch.Size([64, 128, 55, 55])
-Epoch: 0 Step 200 	Learning rate: 0.010000
-Epoch: [0][200/1562]	Time: 1.596 (1.986)	Epoch time [hr]: 0.111 (0.862)	Memory: 5.836 (7.824)	Loss: 5.4293 (5.7135)	Prec@1: 0.000 (0.521)	Prec@5: 0.000 (2.488)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 0.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 411041792.000 bytes
-Optimizer step took: 0.679
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 201 - 201
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 201 - 201
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 201 - 201
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 201 - 201
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 202 - 202
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 202 - 202
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 202 - 202
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 202 - 202
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 203 - 203
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 203 - 203
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 203 - 203
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 203 - 203
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 204 - 204
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 204 - 204
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 204 - 204
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 204 - 204
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 205 - 205
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 205 - 205
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 205 - 205
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 205 - 205
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 206 - 206
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 206 - 206
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 206 - 206
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 206 - 206
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 207 - 207
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 207 - 207
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 207 - 207
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 207 - 207
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 208 - 208
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 208 - 208
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 208 - 208
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 208 - 208
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 209 - 209
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 209 - 209
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 209 - 209
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 209 - 209
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 210 - 210
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 210 - 210
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 210 - 210
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 210 - 210
-recv: tensor: torch.Size([64, 128, 55, 55])
-Epoch: [0][210/1562]	Time: 2.110 (1.990)	Epoch time [hr]: 0.117 (0.863)	Memory: 5.836 (7.824)	Loss: 5.3760 (5.7035)	Prec@1: 0.000 (0.511)	Prec@5: 1.562 (2.473)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 0.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 411041792.000 bytes
-Optimizer step took: 0.661
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 211 - 211
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 211 - 211
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 211 - 211
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 211 - 211
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 212 - 212
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 212 - 212
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 212 - 212
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 212 - 212
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 213 - 213
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 213 - 213
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 213 - 213
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 213 - 213
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 214 - 214
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 214 - 214
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 214 - 214
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 214 - 214
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 215 - 215
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 215 - 215
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 215 - 215
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 215 - 215
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 216 - 216
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 216 - 216
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 216 - 216
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 216 - 216
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 217 - 217
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 217 - 217
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 217 - 217
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 217 - 217
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 218 - 218
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 218 - 218
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 218 - 218
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 218 - 218
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 219 - 219
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 219 - 219
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 219 - 219
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 219 - 219
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 220 - 220
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 220 - 220
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 220 - 220
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 220 - 220
-recv: tensor: torch.Size([64, 128, 55, 55])
-Epoch: [0][220/1562]	Time: 1.974 (1.981)	Epoch time [hr]: 0.122 (0.859)	Memory: 5.836 (7.824)	Loss: 5.3153 (5.6927)	Prec@1: 1.562 (0.509)	Prec@5: 6.250 (2.475)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 0.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 411041792.000 bytes
-Optimizer step took: 0.659
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 221 - 221
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 221 - 221
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 221 - 221
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 221 - 221
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 222 - 222
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 222 - 222
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 222 - 222
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 222 - 222
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 223 - 223
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 223 - 223
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 223 - 223
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 223 - 223
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 224 - 224
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 224 - 224
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 224 - 224
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 224 - 224
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 225 - 225
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 225 - 225
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 225 - 225
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 225 - 225
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 226 - 226
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 226 - 226
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 226 - 226
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 226 - 226
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 227 - 227
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 227 - 227
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 227 - 227
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 227 - 227
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 228 - 228
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 228 - 228
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 228 - 228
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 228 - 228
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 229 - 229
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 229 - 229
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 229 - 229
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 229 - 229
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 230 - 230
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 230 - 230
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 230 - 230
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 230 - 230
-recv: tensor: torch.Size([64, 128, 55, 55])
-Epoch: [0][230/1562]	Time: 1.605 (1.980)	Epoch time [hr]: 0.127 (0.859)	Memory: 5.836 (7.824)	Loss: 5.4925 (5.6814)	Prec@1: 0.000 (0.507)	Prec@5: 1.562 (2.503)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 0.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 411041792.000 bytes
-Optimizer step took: 0.660
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 231 - 231
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 231 - 231
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 231 - 231
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 231 - 231
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 232 - 232
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 232 - 232
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 232 - 232
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 232 - 232
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 233 - 233
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 233 - 233
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 233 - 233
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 233 - 233
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 234 - 234
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 234 - 234
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 234 - 234
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 234 - 234
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 235 - 235
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 235 - 235
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 235 - 235
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 235 - 235
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 236 - 236
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 236 - 236
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 236 - 236
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 236 - 236
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 237 - 237
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 237 - 237
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 237 - 237
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 237 - 237
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 238 - 238
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 238 - 238
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 238 - 238
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 238 - 238
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 239 - 239
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 239 - 239
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 239 - 239
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 239 - 239
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 240 - 240
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 240 - 240
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 240 - 240
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 240 - 240
-recv: tensor: torch.Size([64, 128, 55, 55])
-Epoch: [0][240/1562]	Time: 1.971 (1.978)	Epoch time [hr]: 0.132 (0.858)	Memory: 5.836 (7.824)	Loss: 5.5800 (5.6731)	Prec@1: 0.000 (0.486)	Prec@5: 3.125 (2.483)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 0.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 411041792.000 bytes
-Optimizer step took: 0.660
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 241 - 241
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 241 - 241
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 241 - 241
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 241 - 241
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 242 - 242
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 242 - 242
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 242 - 242
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 242 - 242
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 243 - 243
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 243 - 243
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 243 - 243
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 243 - 243
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 244 - 244
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 244 - 244
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 244 - 244
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 244 - 244
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 245 - 245
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 245 - 245
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 245 - 245
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 245 - 245
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 246 - 246
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 246 - 246
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 246 - 246
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 246 - 246
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 247 - 247
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 247 - 247
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 247 - 247
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 247 - 247
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 248 - 248
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 248 - 248
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 248 - 248
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 248 - 248
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 249 - 249
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 249 - 249
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 249 - 249
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 249 - 249
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 250 - 250
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 250 - 250
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 250 - 250
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 250 - 250
-recv: tensor: torch.Size([64, 128, 55, 55])
-Epoch: [0][250/1562]	Time: 2.112 (1.978)	Epoch time [hr]: 0.138 (0.858)	Memory: 5.836 (7.824)	Loss: 5.4023 (5.6649)	Prec@1: 3.125 (0.492)	Prec@5: 4.688 (2.496)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 0.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 411041792.000 bytes
-Optimizer step took: 0.659
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 251 - 251
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 251 - 251
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 251 - 251
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 251 - 251
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 252 - 252
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 252 - 252
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 252 - 252
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 252 - 252
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 253 - 253
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 253 - 253
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 253 - 253
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 253 - 253
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 254 - 254
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 254 - 254
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 254 - 254
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 254 - 254
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 255 - 255
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 255 - 255
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 255 - 255
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 255 - 255
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 256 - 256
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 256 - 256
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 256 - 256
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 256 - 256
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 257 - 257
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 257 - 257
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 257 - 257
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 257 - 257
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 258 - 258
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 258 - 258
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 258 - 258
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 258 - 258
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 259 - 259
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 259 - 259
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 259 - 259
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 259 - 259
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 260 - 260
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 260 - 260
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 260 - 260
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 260 - 260
-recv: tensor: torch.Size([64, 128, 55, 55])
-Epoch: [0][260/1562]	Time: 1.960 (1.975)	Epoch time [hr]: 0.143 (0.857)	Memory: 5.836 (7.824)	Loss: 5.4495 (5.6577)	Prec@1: 3.125 (0.497)	Prec@5: 3.125 (2.478)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 0.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 411041792.000 bytes
-Optimizer step took: 0.674
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 261 - 261
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 261 - 261
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 261 - 261
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 261 - 261
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 262 - 262
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 262 - 262
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 262 - 262
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 262 - 262
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 263 - 263
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 263 - 263
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 263 - 263
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 263 - 263
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 264 - 264
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 264 - 264
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 264 - 264
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 264 - 264
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 265 - 265
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 265 - 265
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 265 - 265
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 265 - 265
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 266 - 266
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 266 - 266
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 266 - 266
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 266 - 266
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 267 - 267
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 267 - 267
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 267 - 267
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 267 - 267
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 268 - 268
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 268 - 268
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 268 - 268
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 268 - 268
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 269 - 269
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 269 - 269
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 269 - 269
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 269 - 269
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 270 - 270
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 270 - 270
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 270 - 270
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 270 - 270
-recv: tensor: torch.Size([64, 128, 55, 55])
-Epoch: [0][270/1562]	Time: 1.952 (1.975)	Epoch time [hr]: 0.149 (0.857)	Memory: 5.836 (7.824)	Loss: 5.5284 (5.6506)	Prec@1: 1.562 (0.484)	Prec@5: 1.562 (2.439)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 0.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 411041792.000 bytes
-Optimizer step took: 0.652
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 271 - 271
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 271 - 271
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 271 - 271
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 271 - 271
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 272 - 272
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 272 - 272
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 272 - 272
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 272 - 272
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 273 - 273
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 273 - 273
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 273 - 273
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 273 - 273
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 274 - 274
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 274 - 274
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 274 - 274
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 274 - 274
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 275 - 275
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 275 - 275
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 275 - 275
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 275 - 275
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 276 - 276
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 276 - 276
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 276 - 276
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 276 - 276
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 277 - 277
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 277 - 277
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 277 - 277
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 277 - 277
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 278 - 278
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 278 - 278
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 278 - 278
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 278 - 278
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 279 - 279
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 279 - 279
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 279 - 279
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 279 - 279
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 280 - 280
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 280 - 280
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 280 - 280
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 280 - 280
-recv: tensor: torch.Size([64, 128, 55, 55])
-Epoch: [0][280/1562]	Time: 1.616 (1.972)	Epoch time [hr]: 0.154 (0.856)	Memory: 5.836 (7.824)	Loss: 5.4321 (5.6436)	Prec@1: 1.562 (0.495)	Prec@5: 1.562 (2.419)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 0.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 411041792.000 bytes
-Optimizer step took: 0.664
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 281 - 281
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 281 - 281
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 281 - 281
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 281 - 281
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 282 - 282
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 282 - 282
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 282 - 282
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 282 - 282
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 283 - 283
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 283 - 283
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 283 - 283
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 283 - 283
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 284 - 284
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 284 - 284
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 284 - 284
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 284 - 284
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 285 - 285
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 285 - 285
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 285 - 285
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 285 - 285
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 286 - 286
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 286 - 286
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 286 - 286
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 286 - 286
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 287 - 287
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 287 - 287
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 287 - 287
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 287 - 287
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 288 - 288
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 288 - 288
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 288 - 288
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 288 - 288
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 289 - 289
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 289 - 289
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 289 - 289
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 289 - 289
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 290 - 290
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 290 - 290
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 290 - 290
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 290 - 290
-recv: tensor: torch.Size([64, 128, 55, 55])
-Epoch: [0][290/1562]	Time: 1.594 (1.973)	Epoch time [hr]: 0.160 (0.856)	Memory: 5.836 (7.824)	Loss: 5.4950 (5.6378)	Prec@1: 0.000 (0.499)	Prec@5: 4.688 (2.427)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 0.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 411041792.000 bytes
-Optimizer step took: 0.661
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 291 - 291
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 291 - 291
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 291 - 291
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 291 - 291
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 292 - 292
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 292 - 292
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 292 - 292
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 292 - 292
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 293 - 293
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 293 - 293
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 293 - 293
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 293 - 293
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 294 - 294
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 294 - 294
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 294 - 294
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 294 - 294
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 295 - 295
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 295 - 295
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 295 - 295
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 295 - 295
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 296 - 296
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 296 - 296
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 296 - 296
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 296 - 296
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 297 - 297
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 297 - 297
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 297 - 297
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 297 - 297
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 298 - 298
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 298 - 298
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 298 - 298
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 298 - 298
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 299 - 299
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 299 - 299
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 299 - 299
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 299 - 299
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 300 - 300
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 300 - 300
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 300 - 300
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 300 - 300
-recv: tensor: torch.Size([64, 128, 55, 55])
-Epoch: 0 Step 300 	Learning rate: 0.010000
-Epoch: [0][300/1562]	Time: 1.957 (1.969)	Epoch time [hr]: 0.165 (0.854)	Memory: 5.836 (7.824)	Loss: 5.4560 (5.6319)	Prec@1: 0.000 (0.504)	Prec@5: 3.125 (2.414)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 0.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 411041792.000 bytes
-Optimizer step took: 0.661
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 301 - 301
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 301 - 301
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 301 - 301
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 301 - 301
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 302 - 302
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 302 - 302
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 302 - 302
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 302 - 302
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 303 - 303
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 303 - 303
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 303 - 303
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 303 - 303
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 304 - 304
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 304 - 304
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 304 - 304
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 304 - 304
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 305 - 305
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 305 - 305
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 305 - 305
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 305 - 305
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 306 - 306
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 306 - 306
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 306 - 306
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 306 - 306
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 307 - 307
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 307 - 307
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 307 - 307
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 307 - 307
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 308 - 308
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 308 - 308
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 308 - 308
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 308 - 308
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 309 - 309
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 309 - 309
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 309 - 309
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 309 - 309
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 310 - 310
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 310 - 310
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 310 - 310
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 310 - 310
-recv: tensor: torch.Size([64, 128, 55, 55])
-Epoch: [0][310/1562]	Time: 1.624 (1.968)	Epoch time [hr]: 0.170 (0.854)	Memory: 5.836 (7.824)	Loss: 5.4102 (5.6259)	Prec@1: 0.000 (0.497)	Prec@5: 1.562 (2.402)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 0.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 411041792.000 bytes
-Optimizer step took: 0.661
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 311 - 311
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 311 - 311
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 311 - 311
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 311 - 311
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 312 - 312
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 312 - 312
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 312 - 312
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 312 - 312
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 313 - 313
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 313 - 313
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 313 - 313
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 313 - 313
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 314 - 314
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 314 - 314
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 314 - 314
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 314 - 314
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 315 - 315
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 315 - 315
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 315 - 315
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 315 - 315
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 316 - 316
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 316 - 316
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 316 - 316
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 316 - 316
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 317 - 317
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 317 - 317
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 317 - 317
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 317 - 317
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 318 - 318
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 318 - 318
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 318 - 318
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 318 - 318
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 319 - 319
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 319 - 319
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 319 - 319
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 319 - 319
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 320 - 320
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 320 - 320
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 320 - 320
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 320 - 320
-recv: tensor: torch.Size([64, 128, 55, 55])
-Epoch: [0][320/1562]	Time: 2.113 (1.965)	Epoch time [hr]: 0.175 (0.852)	Memory: 5.836 (7.824)	Loss: 5.5035 (5.6202)	Prec@1: 3.125 (0.506)	Prec@5: 4.688 (2.434)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 0.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 411041792.000 bytes
-Optimizer step took: 0.677
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 321 - 321
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 321 - 321
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 321 - 321
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 321 - 321
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 322 - 322
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 322 - 322
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 322 - 322
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 322 - 322
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 323 - 323
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 323 - 323
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 323 - 323
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 323 - 323
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 324 - 324
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 324 - 324
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 324 - 324
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 324 - 324
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 325 - 325
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 325 - 325
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 325 - 325
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 325 - 325
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 326 - 326
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 326 - 326
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 326 - 326
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 326 - 326
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 327 - 327
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 327 - 327
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 327 - 327
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 327 - 327
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 328 - 328
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 328 - 328
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 328 - 328
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 328 - 328
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 329 - 329
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 329 - 329
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 329 - 329
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 329 - 329
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 330 - 330
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 330 - 330
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 330 - 330
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 330 - 330
-recv: tensor: torch.Size([64, 128, 55, 55])
-Epoch: [0][330/1562]	Time: 2.116 (1.966)	Epoch time [hr]: 0.181 (0.853)	Memory: 5.836 (7.824)	Loss: 5.4274 (5.6149)	Prec@1: 1.562 (0.505)	Prec@5: 6.250 (2.436)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 0.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 411041792.000 bytes
-Optimizer step took: 0.659
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 331 - 331
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 331 - 331
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 331 - 331
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 331 - 331
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 332 - 332
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 332 - 332
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 332 - 332
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 332 - 332
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 333 - 333
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 333 - 333
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 333 - 333
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 333 - 333
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 334 - 334
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 334 - 334
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 334 - 334
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 334 - 334
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 335 - 335
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 335 - 335
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 335 - 335
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 335 - 335
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 336 - 336
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 336 - 336
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 336 - 336
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 336 - 336
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 337 - 337
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 337 - 337
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 337 - 337
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 337 - 337
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 338 - 338
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 338 - 338
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 338 - 338
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 338 - 338
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 339 - 339
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 339 - 339
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 339 - 339
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 339 - 339
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 340 - 340
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 340 - 340
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 340 - 340
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 340 - 340
-recv: tensor: torch.Size([64, 128, 55, 55])
-Epoch: [0][340/1562]	Time: 1.951 (1.968)	Epoch time [hr]: 0.186 (0.854)	Memory: 5.836 (7.824)	Loss: 5.3991 (5.6101)	Prec@1: 0.000 (0.504)	Prec@5: 1.562 (2.429)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 0.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 411041792.000 bytes
-Optimizer step took: 0.661
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 341 - 341
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 341 - 341
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 341 - 341
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 341 - 341
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 342 - 342
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 342 - 342
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 342 - 342
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 342 - 342
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 343 - 343
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 343 - 343
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 343 - 343
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 343 - 343
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 344 - 344
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 344 - 344
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 344 - 344
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 344 - 344
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 345 - 345
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 345 - 345
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 345 - 345
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 345 - 345
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 346 - 346
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 346 - 346
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 346 - 346
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 346 - 346
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 347 - 347
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 347 - 347
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 347 - 347
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 347 - 347
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 348 - 348
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 348 - 348
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 348 - 348
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 348 - 348
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 349 - 349
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 349 - 349
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 349 - 349
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 349 - 349
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 350 - 350
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 350 - 350
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 350 - 350
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 350 - 350
-recv: tensor: torch.Size([64, 128, 55, 55])
-Epoch: [0][350/1562]	Time: 2.072 (1.971)	Epoch time [hr]: 0.192 (0.855)	Memory: 5.836 (7.824)	Loss: 5.4475 (5.6052)	Prec@1: 0.000 (0.503)	Prec@5: 0.000 (2.404)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 0.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 411041792.000 bytes
-Optimizer step took: 0.661
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 351 - 351
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 351 - 351
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 351 - 351
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 351 - 351
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 352 - 352
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 352 - 352
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 352 - 352
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 352 - 352
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 353 - 353
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 353 - 353
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 353 - 353
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 353 - 353
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 354 - 354
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 354 - 354
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 354 - 354
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 354 - 354
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 355 - 355
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 355 - 355
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 355 - 355
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 355 - 355
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 356 - 356
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 356 - 356
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 356 - 356
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 356 - 356
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 357 - 357
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 357 - 357
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 357 - 357
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 357 - 357
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 358 - 358
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 358 - 358
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 358 - 358
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 358 - 358
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 359 - 359
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 359 - 359
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 359 - 359
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 359 - 359
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 360 - 360
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 360 - 360
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 360 - 360
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 360 - 360
-recv: tensor: torch.Size([64, 128, 55, 55])
-Epoch: [0][360/1562]	Time: 2.107 (1.966)	Epoch time [hr]: 0.197 (0.853)	Memory: 5.836 (7.824)	Loss: 5.5125 (5.6007)	Prec@1: 0.000 (0.502)	Prec@5: 3.125 (2.394)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 0.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 411041792.000 bytes
-Optimizer step took: 0.661
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 361 - 361
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 361 - 361
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 361 - 361
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 361 - 361
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 362 - 362
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 362 - 362
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 362 - 362
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 362 - 362
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 363 - 363
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 363 - 363
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 363 - 363
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 363 - 363
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 364 - 364
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 364 - 364
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 364 - 364
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 364 - 364
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 365 - 365
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 365 - 365
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 365 - 365
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 365 - 365
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 366 - 366
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 366 - 366
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 366 - 366
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 366 - 366
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 367 - 367
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 367 - 367
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 367 - 367
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 367 - 367
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 368 - 368
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 368 - 368
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 368 - 368
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 368 - 368
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 369 - 369
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 369 - 369
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 369 - 369
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 369 - 369
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 370 - 370
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 370 - 370
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 370 - 370
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 370 - 370
-recv: tensor: torch.Size([64, 128, 55, 55])
-Epoch: [0][370/1562]	Time: 1.949 (1.958)	Epoch time [hr]: 0.202 (0.850)	Memory: 5.836 (7.824)	Loss: 5.4442 (5.5963)	Prec@1: 0.000 (0.497)	Prec@5: 1.562 (2.384)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 0.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 411041792.000 bytes
-Optimizer step took: 0.661
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 371 - 371
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 371 - 371
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 371 - 371
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 371 - 371
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 372 - 372
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 372 - 372
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 372 - 372
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 372 - 372
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 373 - 373
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 373 - 373
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 373 - 373
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 373 - 373
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 374 - 374
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 374 - 374
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 374 - 374
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 374 - 374
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 375 - 375
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 375 - 375
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 375 - 375
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 375 - 375
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 376 - 376
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 376 - 376
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 376 - 376
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 376 - 376
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 377 - 377
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 377 - 377
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 377 - 377
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 377 - 377
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 378 - 378
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 378 - 378
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 378 - 378
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 378 - 378
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 379 - 379
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 379 - 379
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 379 - 379
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 379 - 379
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 380 - 380
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 380 - 380
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 380 - 380
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 380 - 380
-recv: tensor: torch.Size([64, 128, 55, 55])
-Epoch: [0][380/1562]	Time: 2.122 (1.957)	Epoch time [hr]: 0.207 (0.849)	Memory: 5.836 (7.824)	Loss: 5.2776 (5.5915)	Prec@1: 1.562 (0.504)	Prec@5: 1.562 (2.387)
-Backward Stats:
-	 compute_time 0.000 seconds
-	 receive_tensors 0.000 seconds
-	 receive_tensors_size 0.000 bytes
-	 send_tensors 0.000 seconds
-	 send_tensors_size 411041792.000 bytes
-Optimizer step took: 0.661
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 381 - 381
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 381 - 381
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 381 - 381
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 381 - 381
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 382 - 382
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 382 - 382
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 382 - 382
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 382 - 382
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 383 - 383
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 383 - 383
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 383 - 383
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 383 - 383
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 384 - 384
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 384 - 384
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 384 - 384
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 384 - 384
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 385 - 385
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 385 - 385
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 385 - 385
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 385 - 385
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 386 - 386
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 386 - 386
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 386 - 386
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 386 - 386
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 387 - 387
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 387 - 387
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 387 - 387
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 387 - 387
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 388 - 388
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 388 - 388
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 388 - 388
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 388 - 388
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 389 - 389
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 389 - 389
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 389 - 389
-recv: tensor: torch.Size([64, 128, 55, 57])
-enter recv_block f_mini-b_mini: 389 - 389
-recv: tensor: torch.Size([64, 128, 55, 55])
-enter run_forward
-skip recv: out0
-try recv: target
-recv: target
-enter _run_forward
-Start downstream head layer
-enter recv_block f_mini-b_mini: 390 - 390
-recv: tensor: torch.Size([64, 128, 57, 57])
-enter recv_block f_mini-b_mini: 390 - 390
-recv: tensor: torch.Size([64, 128, 57, 55])
-enter recv_block f_mini-b_mini: 390 - 390
diff --git a/run.sh b/run.sh
index ee94ad9..a842b63 100755
--- a/run.sh
+++ b/run.sh
@@ -4,6 +4,12 @@ ROOT=/home/ubuntu/pipedream/runtime
 YAML_FILE=vgg16_2mp_exp.yml
 
 
+if (( $(ifconfig | grep $LOCAL_IP | wc -l) == 1 )); then
+    echo "running run.sh on master"
+else
+    echo "running run.sh on slave"
+fi
+
 sed -i "s/HOST0/$LOCAL_IP/g" $ROOT/image_classification/driver_configs/$YAML_FILE
 sed -i "s/HOST1/$REMOTE_IP/g" $ROOT/image_classification/driver_configs/$YAML_FILE
 
@@ -41,4 +47,7 @@ fi
 if [ "$1" = "rlog" ]; then
     cd $LOG_DIRNAME
     ssh -n $REMOTE_IP -o StrictHostKeyChecking=no "cd $LOG_DIRNAME && tail -f output.log.1"
-fi
\ No newline at end of file
+fi
+
+sed -i "s/$LOCAL_IP/HOST0/g" $ROOT/image_classification/driver_configs/$YAML_FILE
+sed -i "s/$REMOTE_IP/HOST1/g" $ROOT/image_classification/driver_configs/$YAML_FILE
diff --git a/runtime/image_classification/driver_configs/vgg16_2mp_exp.yml b/runtime/image_classification/driver_configs/vgg16_2mp_exp.yml
index 9071b4a..e2428dc 100644
--- a/runtime/image_classification/driver_configs/vgg16_2mp_exp.yml
+++ b/runtime/image_classification/driver_configs/vgg16_2mp_exp.yml
@@ -3,7 +3,7 @@
 'data_dir': '/home/ubuntu/data/imagenet_tiny/'
 'config_file': 'models/vgg16/gpus=2/mp_conf.json'
 'container': 'nvcr.io/nvidia/pytorch:19.05-py3'
-'machines': ['HOST0:0', 'HOST1:1']
+'machines': ['172.31.65.131:0', '172.31.76.4:1']
 'batch_size': 64
 'learning_rate': 0.01
 'weight_decay': 0.0005
diff --git a/runtime/image_classification/launch.py b/runtime/image_classification/launch.py
deleted file mode 100644
index d167d3f..0000000
--- a/runtime/image_classification/launch.py
+++ /dev/null
@@ -1,173 +0,0 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT license.
-
-r"""
-`torch.distributed.launch` is a module that spawns up multiple distributed
-training processes on each of the training nodes.
-The utility can be used for single-node distributed training, in which one or
-more processes per node will be spawned. The utility can be used for either
-CPU training or GPU training. If the utility is used for GPU training,
-each distributed process will be operating on a single GPU. This can achieve
-well-improved single-node training performance. It can also be used in
-multi-node distributed training, by spawning up multiple processes on each node
-for well-improved multi-node distributed training performance as well.
-This will especially be benefitial for systems with multiple Infiniband
-interfaces that have direct-GPU support, since all of them can be utilized for
-aggregated communication bandwidth.
-In both cases of single-node distributed training or multi-node distributed
-training, this utility will launch the given number of processes per node
-(``--nproc_per_node``). If used for GPU training, this number needs to be less
-or equal to the number of GPUs on the current system (``nproc_per_node``),
-and each process will be operating on a single GPU from *GPU 0 to
-GPU (nproc_per_node - 1)*.
-**How to use this module:**
-1. Single-Node multi-process distributed training
-::
-    >>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE
-               YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other
-               arguments of your training script)
-2. Multi-Node multi-process distributed training: (e.g. two nodes)
-Node 1: *(IP: 192.168.1.1, and has a free port: 1234)*
-::
-    >>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE
-               --nnodes=2 --node_rank=0 --master_addr="192.168.1.1"
-               --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3
-               and all other arguments of your training script)
-Node 2:
-::
-    >>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE
-               --nnodes=2 --node_rank=1 --master_addr="192.168.1.1"
-               --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3
-               and all other arguments of your training script)
-3. To look up what optional arguments this module offers:
-::
-    >>> python -m torch.distributed.launch --help
-**Important Notices:**
-1. This utilty and multi-process distributed (single-node or
-multi-node) GPU training currently only achieves the best performance using
-the NCCL distributed backend. Thus NCCL backend is the recommended backend to
-use for GPU training.
-2. In your training program, you must parse the command-line argument:
-``--local_rank=LOCAL_PROCESS_RANK``, which will be provided by this module.
-If your training program uses GPUs, you should ensure that your code only
-runs on the GPU device of LOCAL_PROCESS_RANK. This can be done by:
-Parsing the local_rank argument
-::
-    >>> import argparse
-    >>> parser = argparse.ArgumentParser()
-    >>> parser.add_argument("--local_rank", type=int)
-    >>> args = parser.parse_args()
-Set your device to local rank using either
-::
-    >>> torch.cuda.set_device(arg.local_rank)  # before your code runs
-or
-::
-    >>> with torch.cuda.device(arg.local_rank):
-    >>>    # your code to run
-3. In your training program, you are supposed to call the following function
-at the beginning to start the distributed backend. You need to make sure that
-the init_method uses ``env://``, which is the only supported ``init_method``
-by this module.
-::
-    torch.distributed.init_process_group(backend='YOUR BACKEND',
-                                         init_method='env://')
-4. In your training program, you can either use regular distributed functions
-or use :func:`torch.nn.parallel.DistributedDataParallel` module. If your
-training program uses GPUs for training and you would like to use
-:func:`torch.nn.parallel.DistributedDataParallel` module,
-here is how to configure it.
-::
-    model = torch.nn.parallel.DistributedDataParallel(model,
-                                                      device_ids=[arg.local_rank],
-                                                      output_device=arg.local_rank)
-Please ensure that ``device_ids`` argument is set to be the only GPU device id
-that your code will be operating on. This is generally the local rank of the
-process. In other words, the ``device_ids`` needs to be ``[args.local_rank]``,
-and ``output_device`` needs to be ``args.local_rank`` in order to use this
-utility
-5. Another way to pass ``local_rank`` to the subprocesses via environment variable
-``LOCAL_RANK``. This behavior is enabled when you launch the script with
-``--use_env=True``. You must adjust the subprocess example above to replace
-``args.local_rank`` with ``os.environ['LOCAL_RANK']``; the launcher
-will not pass ``--local_rank`` when you specify this flag.
-.. warning::
-    ``local_rank`` is NOT globally unique: it is only unique per process
-    on a machine.  Thus, don't use it to decide if you should, e.g.,
-    write to a networked filesystem.  See
-    https://github.com/pytorch/pytorch/issues/12042 for an example of
-    how things can go wrong if you don't do this correctly.
-"""
-
-
-import sys
-import subprocess
-import os
-from argparse import ArgumentParser, REMAINDER
-
-
-def parse_args():
-    """
-    Helper function parsing the command line options
-    @retval ArgumentParser
-    """
-    parser = ArgumentParser(description="PyTorch distributed training launch "
-                                        "helper utilty that will spawn up "
-                                        "multiple distributed processes")
-
-    # Optional arguments for the launch helper
-    parser.add_argument("--nnodes", type=int, default=1,
-                        help="The number of nodes to use for distributed "
-                             "training")
-    parser.add_argument("--node_rank", type=int, default=0,
-                        help="The rank of the node for multi-node distributed "
-                             "training")
-    parser.add_argument("--nproc_per_node", type=int, default=1,
-                        help="The number of processes to launch on each node, "
-                             "for GPU training, this is recommended to be set "
-                             "to the number of GPUs in your system so that "
-                             "each process can be bound to a single GPU.")
-
-    # positional
-    parser.add_argument("training_script", type=str,
-                        help="The full path to the single GPU training "
-                             "program/script to be launched in parallel, "
-                             "followed by all the arguments for the "
-                             "training script")
-
-    # rest from the training program
-    parser.add_argument('training_script_args', nargs=REMAINDER)
-    return parser.parse_args()
-
-
-def main():
-    args = parse_args()
-
-    # world size in terms of number of processes
-    dist_world_size = args.nproc_per_node * args.nnodes
-
-    # set PyTorch distributed related environmental variables
-    processes = []
-
-    for local_rank in range(0, args.nproc_per_node):
-        # each process's rank
-        dist_rank = args.nproc_per_node * args.node_rank + local_rank
-
-        # spawn the processes
-        cmd = [sys.executable,
-               "-u",
-               args.training_script,
-               "--rank={}".format(dist_rank),
-               "--local_rank={}".format(local_rank)] + args.training_script_args
-
-        process = subprocess.Popen(cmd)
-        processes.append(process)
-
-    for process in processes:
-        process.wait()
-        if process.returncode != 0:
-            raise subprocess.CalledProcessError(returncode=process.returncode,
-                                                cmd=cmd)
-
-
-if __name__ == "__main__":
-    main()
diff --git a/runtime/image_classification/models/vgg16/gpus=2/stage0.py b/runtime/image_classification/models/vgg16/gpus=2/stage0.py
index 809d7da..6158aa3 100644
--- a/runtime/image_classification/models/vgg16/gpus=2/stage0.py
+++ b/runtime/image_classification/models/vgg16/gpus=2/stage0.py
@@ -2,7 +2,7 @@
 # Licensed under the MIT license.
 
 import torch
-
+from datetime import datetime
 
 class Stage0(torch.nn.Module):
     def __init__(self):
@@ -19,6 +19,8 @@ class Stage0(torch.nn.Module):
         self._initialize_weights()
 
     def forward(self, input0, forward_minibatch_id, backward_minibatch_id, comm_handler):
+        start_time = datetime.now()
+    
         out0 = input0.clone()
         out2 = self.layer2(out0)
         out3 = self.layer3(out2)
@@ -27,7 +29,19 @@ class Stage0(torch.nn.Module):
         out6 = self.layer6(out5)
         out7 = self.layer7(out6)
         out8 = self.layer8(out7)
+
+        dt = datetime.now() - start_time
+        elapsed = (dt.days * 24 * 60 * 60 + dt.seconds) * 1000 + dt.microseconds / 1000.0
+        print("Stage0 before last layer:", "%.20fms" % elapsed)
+
+        start_time = datetime.now()
+        
         out9 = self.upstream_tail(out8, forward_minibatch_id, backward_minibatch_id, comm_handler)
+        
+        dt = datetime.now() - start_time
+        elapsed = (dt.days * 24 * 60 * 60 + dt.seconds) * 1000 + dt.microseconds / 1000.0
+        print("Stage0 last layer:", "%.20fms" % elapsed)
+        
         return out9
 
     def _initialize_weights(self):
@@ -58,16 +72,14 @@ class Upstream_Tail(torch.nn.Module):
     def forward(self, inp, forward_minibatch_id, backward_minibatch_id, comm_handler):
         
         block_out_list = []
-        block_num = 4
-        
-        batch_size, c_in = inp.shape[0], inp.shape[1]
-        h_i, w_i = inp.shape[2], inp.shape[3]
-        
+                
         inp = self.padder(inp)
         h_pad, w_pad = inp.size(2), inp.size(3)
         block_height, block_width = h_pad // 2,  w_pad // 2
         
         # block_0
+        start_time = datetime.now()
+
         h_start, h_end = 0, block_height + self.kernel_size-1
         w_start, w_end = 0, block_width + self.kernel_size-1
 
@@ -79,7 +91,13 @@ class Upstream_Tail(torch.nn.Module):
         comm_handler.send_block(block_out, forward_minibatch_id=forward_minibatch_id,
                                      backward_minibatch_id=backward_minibatch_id)
 
+        dt = datetime.now() - start_time
+        elapsed = (dt.days * 24 * 60 * 60 + dt.seconds) * 1000 + dt.microseconds / 1000.0
+        print(" ->block 0 time:", "%.20fms" % elapsed)
+
         # block_1
+        start_time = datetime.now()
+
         h_start, h_end = 0, block_height + self.kernel_size-1
         w_start, w_end = block_width, w_pad
 
@@ -91,7 +109,13 @@ class Upstream_Tail(torch.nn.Module):
         comm_handler.send_block(block_out, forward_minibatch_id=forward_minibatch_id,
                                      backward_minibatch_id=backward_minibatch_id)
 
+        dt = datetime.now() - start_time
+        elapsed = (dt.days * 24 * 60 * 60 + dt.seconds) * 1000 + dt.microseconds / 1000.0
+        print(" ->block 1 time:", "%.20fms" % elapsed)
+
         # block_2
+        start_time = datetime.now()
+
         h_start, h_end = block_height, h_pad
         w_start, w_end = 0, block_width + self.kernel_size-1
 
@@ -103,7 +127,13 @@ class Upstream_Tail(torch.nn.Module):
         comm_handler.send_block(block_out, forward_minibatch_id=forward_minibatch_id,
                              backward_minibatch_id=backward_minibatch_id)
         
+        dt = datetime.now() - start_time
+        elapsed = (dt.days * 24 * 60 * 60 + dt.seconds) * 1000 + dt.microseconds / 1000.0
+        print(" ->block 2 time:", "%.20fms" % elapsed)
+
         # block_3
+        start_time = datetime.now()
+
         h_start, h_end = block_height, h_pad
         w_start, w_end = block_width, w_pad
 
@@ -115,6 +145,10 @@ class Upstream_Tail(torch.nn.Module):
         comm_handler.send_block(block_out, forward_minibatch_id=forward_minibatch_id,
                                      backward_minibatch_id=backward_minibatch_id)
 
+        dt = datetime.now() - start_time
+        elapsed = (dt.days * 24 * 60 * 60 + dt.seconds) * 1000 + dt.microseconds / 1000.0
+        print(" ->block 3 time:", "%.20fms" % elapsed)
+
         return self._combine(block_out_list)
     
     def _combine(self, block_list):
diff --git a/runtime/image_classification/models/vgg16/gpus=2/stage1.py b/runtime/image_classification/models/vgg16/gpus=2/stage1.py
index 27435d3..cf70a48 100644
--- a/runtime/image_classification/models/vgg16/gpus=2/stage1.py
+++ b/runtime/image_classification/models/vgg16/gpus=2/stage1.py
@@ -2,7 +2,7 @@
 # Licensed under the MIT license.
 
 import torch
-
+from datetime import datetime
 
 class Stage1(torch.nn.Module):
     def __init__(self):
@@ -41,7 +41,15 @@ class Stage1(torch.nn.Module):
         self._initialize_weights()
 
     def forward(self, forward_minibatch_id, backward_minibatch_id, r):
+        start_time = datetime.now()
+        
         out1 = self.downstream_head(forward_minibatch_id, backward_minibatch_id, r)
+        
+        dt = datetime.now() - start_time
+        elapsed = (dt.days * 24 * 60 * 60 + dt.seconds) * 1000 + dt.microseconds / 1000.0
+        print("Stage1 1st layer:", "%.20fms" % elapsed)
+
+        start_time = datetime.now()
         out2 = self.layer2(out1)
         out3 = self.layer3(out2)
         out4 = self.layer4(out3)
@@ -73,6 +81,11 @@ class Stage1(torch.nn.Module):
         out30 = self.layer30(out29)
         out31 = self.layer31(out30)
         out32 = self.layer32(out31)
+
+        dt = datetime.now() - start_time
+        elapsed = (dt.days * 24 * 60 * 60 + dt.seconds) * 1000 + dt.microseconds / 1000.0
+        print("Stage1 other layers:", "%.20fms" % elapsed)
+
         return out32
 
     def _initialize_weights(self):
@@ -99,14 +112,21 @@ class Downstream_Head(torch.nn.Module):
         block_out_relu = []
         
         for block_id in range(block_num):
+            start_time = datetime.now()
             block_inp_relu = r.comm_handler.recv_block(forward_minibatch_id, backward_minibatch_id)
+            dt = datetime.now() - start_time
+            elapsed = (dt.days * 24 * 60 * 60 + dt.seconds) * 1000 + dt.microseconds / 1000.0
+            print(" ->bid:", block_id, "recv elapsed:", "%.20fms" % elapsed)
+
             # store block_inp_relu into buffer
             # slice and clone buffer and pass into ReLU
             # return buffer as input_tensor
+
+            start_time = datetime.now()
             if (block_id == 0):
                 # infer shape from the first recv block
                 batch_size, channel_size = block_inp_relu.size(0), block_inp_relu.size(1)
-                block_buffer = torch.zeros(batch_size, channel_size, 112, 112).to("cuda")
+                block_buffer = torch.cuda.FloatTensor(batch_size, channel_size, 112, 112).fill_(0)
 
                 block_buffer[:, :, :57, :57] = block_inp_relu
                 block_out_relu.append(self.relu(block_buffer[:, :, :57, :57].clone()))
@@ -119,13 +139,20 @@ class Downstream_Head(torch.nn.Module):
             else:
                 block_buffer[:, :, 57:, 57:] = block_inp_relu
                 block_out_relu.append(self.relu(block_buffer[:, :, 57:, 57:].clone()))
+            dt = datetime.now() - start_time
+            elapsed = (dt.days * 24 * 60 * 60 + dt.seconds) * 1000 + dt.microseconds / 1000.0
+            print(" ->bid:", block_id, "fill elapsed:", "%.20fms" % elapsed)
 
 
         # Used to track where to receive forward from.
         r.comm_handler.increment_messaging_index(
             sending=False)
         
+        start_time = datetime.now()
         relu_out = self._combine(block_out_relu)
+        dt = datetime.now() - start_time
+        elapsed = (dt.days * 24 * 60 * 60 + dt.seconds) * 1000 + dt.microseconds / 1000.0
+        print(" ->_combine elapsed:", "%.20fms" % elapsed)
 
         r.tensors[-1]["out0"] = block_buffer
 
diff --git a/runtime/runtime_block.py b/runtime/runtime_block.py
index 7519320..15d28e1 100644
--- a/runtime/runtime_block.py
+++ b/runtime/runtime_block.py
@@ -506,22 +506,34 @@ class StageRuntime:
         """Run forward pass.
         """
         # Receive tensors from previous worker.
-        start_time = time.time()
+        from datetime import datetime
+
+        print("forward_minibatch_id", self.forward_minibatch_id)
+
+        start_time = datetime.now()
         self.receive_tensors_forward()
         tensors = self.tensors[-1]
 
+        dt = datetime.now() - start_time
+        elapsed = (dt.days * 24 * 60 * 60 + dt.seconds) * 1000 + dt.microseconds / 1000.0
+        print(" -> recv elapsed:", "%.20fms" % elapsed)
+
+        start_time = datetime.now()
+
         # Run forward pass.
         self._run_forward(tensors)
 
+        dt = datetime.now() - start_time
+        elapsed = (dt.days * 24 * 60 * 60 + dt.seconds) * 1000 + dt.microseconds / 1000.0
+        print(" -> _run_forward elapsed:", "%.20fms" % elapsed)
+        print("")
+
         # Send tensors forward.
         self.send_tensors_forward()
         # if self.verbose_freq > 0 and self.forward_minibatch_id % self.verbose_freq == 0:
         #     self.forward_stats.print_stats()
         # self.forward_stats.reset_stats()
 
-        elapsed = time.time() - start_time
-        print("forward_minibatch_id", self.forward_minibatch_id, "run_forward elapsed:", "%.20f" % elapsed)
-
         self.forward_minibatch_id += 1
 
     def _run_forward(self, tensors):
